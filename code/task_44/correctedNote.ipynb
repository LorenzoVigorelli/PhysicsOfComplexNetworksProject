{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4affe6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SCI (TSV) – prime righe ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_loc</th>\n",
       "      <th>fr_loc</th>\n",
       "      <th>scaled_sci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABW</td>\n",
       "      <td>ABW</td>\n",
       "      <td>11264841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO1</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO10</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO11</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO12</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_loc fr_loc  scaled_sci\n",
       "0      ABW    ABW  11264841.0\n",
       "1      ABW   AGO1        38.0\n",
       "2      ABW  AGO10        34.0\n",
       "3      ABW  AGO11        32.0\n",
       "4      ABW  AGO12        23.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "user_loc      string[python]\n",
      "fr_loc        string[python]\n",
      "scaled_sci           float64\n",
      "dtype: object\n",
      "\n",
      "Numero righe: 63,824,121\n",
      "== Mapping livelli – prime righe ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_code</th>\n",
       "      <th>level_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AND</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATG</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABW</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BHS</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRB</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_code level_type\n",
       "0           AND    country\n",
       "1           ATG    country\n",
       "2           ABW    country\n",
       "3           BHS    country\n",
       "4           BRB    country"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "location_code    string[python]\n",
      "level_type       string[python]\n",
      "dtype: object\n",
      "\n",
      "Numero righe: 8,008\n",
      "\n",
      "== level_type value_counts ==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "level_type\n",
       "county     3229\n",
       "gadm1      1839\n",
       "nuts3      1522\n",
       "gadm2      1370\n",
       "country      48\n",
       "Name: count, dtype: Int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Copertura NODI ==\n",
      "- total_unique_codes: 7989\n",
      "- mapped_unique_codes: 7984\n",
      "- unmapped_unique_codes: 5\n",
      "- node_coverage_pct: 99.93741394417324\n",
      "\n",
      "Esempi codici NON mappati (5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VIR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_code\n",
       "0           ASM\n",
       "1           GUM\n",
       "2           MNP\n",
       "3          MUS1\n",
       "4           VIR"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Copertura ARCHI ==\n",
      "- total_rows: 63824121\n",
      "- valid_rows_both_mapped: 63744256\n",
      "- edge_coverage_pct: 99.87486705849031\n",
      "[AVVISO] Colonna 'country_ISO3' assente nel mapping: copertura per paese non calcolata.\n",
      "\n",
      "== Copertura PER PAESE ==\n",
      "Mapping senza 'country_ISO3' → se vuoi questa sezione, aggiungi la colonna a df_map.\n",
      "Layer trovati in data/gadm_410.gpkg:\n",
      " 1. gadm_410\n",
      "\n",
      "Colonne disponibili in NUTS3 GeoJSON:\n",
      "['LEVL_CODE', 'NUTS_ID', 'CNTR_CODE', 'NAME_LATN', 'NUTS_NAME', 'MOUNT_TYPE', 'URBN_TYPE', 'COAST_TYPE', 'geometry']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LEVL_CODE</th>\n",
       "      <th>NUTS_ID</th>\n",
       "      <th>CNTR_CODE</th>\n",
       "      <th>NAME_LATN</th>\n",
       "      <th>NUTS_NAME</th>\n",
       "      <th>MOUNT_TYPE</th>\n",
       "      <th>URBN_TYPE</th>\n",
       "      <th>COAST_TYPE</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>CZ052</td>\n",
       "      <td>CZ</td>\n",
       "      <td>Královéhradecký kraj</td>\n",
       "      <td>Královéhradecký kraj</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((16.10732 50.66207, 16.33255 50.59246...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>CZ053</td>\n",
       "      <td>CZ</td>\n",
       "      <td>Pardubický kraj</td>\n",
       "      <td>Pardubický kraj</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((16.8042 49.59881, 16.39363 49.58061,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CZ063</td>\n",
       "      <td>CZ</td>\n",
       "      <td>Kraj Vysočina</td>\n",
       "      <td>Kraj Vysočina</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((16.39363 49.58061, 16.25967 49.27462...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CZ064</td>\n",
       "      <td>CZ</td>\n",
       "      <td>Jihomoravský kraj</td>\n",
       "      <td>Jihomoravský kraj</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((17.15943 49.27462, 17.27319 49.05789...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>CZ071</td>\n",
       "      <td>CZ</td>\n",
       "      <td>Olomoucký kraj</td>\n",
       "      <td>Olomoucký kraj</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>POLYGON ((17.4296 50.25451, 17.17647 49.95354,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LEVL_CODE NUTS_ID CNTR_CODE             NAME_LATN             NUTS_NAME  \\\n",
       "0          3   CZ052        CZ  Královéhradecký kraj  Královéhradecký kraj   \n",
       "1          3   CZ053        CZ       Pardubický kraj       Pardubický kraj   \n",
       "2          3   CZ063        CZ         Kraj Vysočina         Kraj Vysočina   \n",
       "3          3   CZ064        CZ     Jihomoravský kraj     Jihomoravský kraj   \n",
       "4          3   CZ071        CZ        Olomoucký kraj        Olomoucký kraj   \n",
       "\n",
       "   MOUNT_TYPE  URBN_TYPE  COAST_TYPE  \\\n",
       "0           4          2           3   \n",
       "1           4          3           3   \n",
       "2           4          3           3   \n",
       "3           4          2           3   \n",
       "4           2          2           3   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((16.10732 50.66207, 16.33255 50.59246...  \n",
       "1  POLYGON ((16.8042 49.59881, 16.39363 49.58061,...  \n",
       "2  POLYGON ((16.39363 49.58061, 16.25967 49.27462...  \n",
       "3  POLYGON ((17.15943 49.27462, 17.27319 49.05789...  \n",
       "4  POLYGON ((17.4296 50.25451, 17.17647 49.95354,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping field geo_point_2d: unsupported OGR type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colonne disponibili in US counties GeoJSON:\n",
      "['intptlat', 'countyfp_nozero', 'countyns', 'stusab', 'csafp', 'state_name', 'aland', 'geoid', 'namelsad', 'countyfp', 'awater', 'classfp', 'lsad', 'name', 'funcstat', 'metdivfp', 'cbsafp', 'intptlon', 'statefp', 'mtfcc', 'geometry']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intptlat</th>\n",
       "      <th>countyfp_nozero</th>\n",
       "      <th>countyns</th>\n",
       "      <th>stusab</th>\n",
       "      <th>csafp</th>\n",
       "      <th>state_name</th>\n",
       "      <th>aland</th>\n",
       "      <th>geoid</th>\n",
       "      <th>namelsad</th>\n",
       "      <th>countyfp</th>\n",
       "      <th>...</th>\n",
       "      <th>classfp</th>\n",
       "      <th>lsad</th>\n",
       "      <th>name</th>\n",
       "      <th>funcstat</th>\n",
       "      <th>metdivfp</th>\n",
       "      <th>cbsafp</th>\n",
       "      <th>intptlon</th>\n",
       "      <th>statefp</th>\n",
       "      <th>mtfcc</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+43.0066030</td>\n",
       "      <td>135</td>\n",
       "      <td>01266975</td>\n",
       "      <td>SD</td>\n",
       "      <td>None</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>1349873585</td>\n",
       "      <td>46135</td>\n",
       "      <td>Yankton County</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>H1</td>\n",
       "      <td>06</td>\n",
       "      <td>Yankton</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>49460</td>\n",
       "      <td>-097.3883614</td>\n",
       "      <td>46</td>\n",
       "      <td>G4020</td>\n",
       "      <td>POLYGON ((-97.51843 43.16903, -97.49807 43.169...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+41.5929185</td>\n",
       "      <td>49</td>\n",
       "      <td>00277289</td>\n",
       "      <td>CA</td>\n",
       "      <td>None</td>\n",
       "      <td>California</td>\n",
       "      <td>10225096402</td>\n",
       "      <td>06049</td>\n",
       "      <td>Modoc County</td>\n",
       "      <td>049</td>\n",
       "      <td>...</td>\n",
       "      <td>H1</td>\n",
       "      <td>06</td>\n",
       "      <td>Modoc</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-120.7183704</td>\n",
       "      <td>06</td>\n",
       "      <td>G4020</td>\n",
       "      <td>POLYGON ((-121.4489 41.47281, -121.44891 41.47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+32.2388026</td>\n",
       "      <td>235</td>\n",
       "      <td>00347593</td>\n",
       "      <td>GA</td>\n",
       "      <td>None</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>645583957</td>\n",
       "      <td>13235</td>\n",
       "      <td>Pulaski County</td>\n",
       "      <td>235</td>\n",
       "      <td>...</td>\n",
       "      <td>H1</td>\n",
       "      <td>06</td>\n",
       "      <td>Pulaski</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-083.4818454</td>\n",
       "      <td>13</td>\n",
       "      <td>G4020</td>\n",
       "      <td>POLYGON ((-83.6065 32.26751, -83.60621 32.2756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+39.1642619</td>\n",
       "      <td>13</td>\n",
       "      <td>00424208</td>\n",
       "      <td>IL</td>\n",
       "      <td>476</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>657422422</td>\n",
       "      <td>17013</td>\n",
       "      <td>Calhoun County</td>\n",
       "      <td>013</td>\n",
       "      <td>...</td>\n",
       "      <td>H1</td>\n",
       "      <td>06</td>\n",
       "      <td>Calhoun</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>41180</td>\n",
       "      <td>-090.6662949</td>\n",
       "      <td>17</td>\n",
       "      <td>G4020</td>\n",
       "      <td>POLYGON ((-90.71598 39.19147, -90.716 39.19155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+30.2064437</td>\n",
       "      <td>5</td>\n",
       "      <td>00558403</td>\n",
       "      <td>LA</td>\n",
       "      <td>None</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>751259388</td>\n",
       "      <td>22005</td>\n",
       "      <td>Ascension Parish</td>\n",
       "      <td>005</td>\n",
       "      <td>...</td>\n",
       "      <td>H1</td>\n",
       "      <td>15</td>\n",
       "      <td>Ascension</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>12940</td>\n",
       "      <td>-090.9125023</td>\n",
       "      <td>22</td>\n",
       "      <td>G4020</td>\n",
       "      <td>POLYGON ((-91.0122 30.33565, -91.0118 30.33575...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      intptlat countyfp_nozero  countyns stusab csafp    state_name  \\\n",
       "0  +43.0066030             135  01266975     SD  None  South Dakota   \n",
       "1  +41.5929185              49  00277289     CA  None    California   \n",
       "2  +32.2388026             235  00347593     GA  None       Georgia   \n",
       "3  +39.1642619              13  00424208     IL   476      Illinois   \n",
       "4  +30.2064437               5  00558403     LA  None     Louisiana   \n",
       "\n",
       "         aland  geoid          namelsad countyfp  ...  classfp lsad  \\\n",
       "0   1349873585  46135    Yankton County      135  ...       H1   06   \n",
       "1  10225096402  06049      Modoc County      049  ...       H1   06   \n",
       "2    645583957  13235    Pulaski County      235  ...       H1   06   \n",
       "3    657422422  17013    Calhoun County      013  ...       H1   06   \n",
       "4    751259388  22005  Ascension Parish      005  ...       H1   15   \n",
       "\n",
       "        name funcstat metdivfp cbsafp      intptlon statefp  mtfcc  \\\n",
       "0    Yankton        A     None  49460  -097.3883614      46  G4020   \n",
       "1      Modoc        A     None   None  -120.7183704      06  G4020   \n",
       "2    Pulaski        A     None   None  -083.4818454      13  G4020   \n",
       "3    Calhoun        A     None  41180  -090.6662949      17  G4020   \n",
       "4  Ascension        A     None  12940  -090.9125023      22  G4020   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-97.51843 43.16903, -97.49807 43.169...  \n",
       "1  POLYGON ((-121.4489 41.47281, -121.44891 41.47...  \n",
       "2  POLYGON ((-83.6065 32.26751, -83.60621 32.2756...  \n",
       "3  POLYGON ((-90.71598 39.19147, -90.716 39.19155...  \n",
       "4  POLYGON ((-91.0122 30.33565, -91.0118 30.33575...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Caricato layer='gadm_410' con 356,508 feature\n",
      "[INFO] CRS: EPSG:4326\n",
      "[INFO] Colonne disponibili:\n",
      "['UID', 'GID_0', 'NAME_0', 'VARNAME_0', 'GID_1', 'NAME_1', 'VARNAME_1', 'NL_NAME_1', 'ISO_1', 'HASC_1', 'CC_1', 'TYPE_1', 'ENGTYPE_1', 'VALIDFR_1', 'GID_2', 'NAME_2', 'VARNAME_2', 'NL_NAME_2', 'HASC_2', 'CC_2', 'TYPE_2', 'ENGTYPE_2', 'VALIDFR_2', 'GID_3', 'NAME_3', 'VARNAME_3', 'NL_NAME_3', 'HASC_3', 'CC_3', 'TYPE_3', 'ENGTYPE_3', 'VALIDFR_3', 'GID_4', 'NAME_4', 'VARNAME_4', 'CC_4', 'TYPE_4', 'ENGTYPE_4', 'VALIDFR_4', 'GID_5', 'NAME_5', 'CC_5', 'TYPE_5', 'ENGTYPE_5', 'GOVERNEDBY', 'SOVEREIGN', 'DISPUTEDBY', 'REGION', 'VARREGION', 'COUNTRY', 'CONTINENT', 'SUBCONT', 'geometry']\n",
      "[INFO] Colonna codice ADM2 individuata: 'GID_2'\n",
      "[INFO] Righe ADM2 (non-NaN su GID_2): 356,508\n",
      "[INFO] Codici ADM2 unici: 47,218\n",
      "[INFO] Geometrie (conteggio per tipo): {'MultiPolygon': 356508}\n",
      "\n",
      "== Anteprima ADM2 (prime 5 righe) ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID_2</th>\n",
       "      <th>NAME_0</th>\n",
       "      <th>NAME_1</th>\n",
       "      <th>NAME_2</th>\n",
       "      <th>GID_0</th>\n",
       "      <th>GID_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG.1.1_1</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Baharak</td>\n",
       "      <td>AFG</td>\n",
       "      <td>AFG.1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG.1.2_1</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Darwaz</td>\n",
       "      <td>AFG</td>\n",
       "      <td>AFG.1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG.1.3_1</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Fayzabad</td>\n",
       "      <td>AFG</td>\n",
       "      <td>AFG.1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG.1.4_1</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Ishkashim</td>\n",
       "      <td>AFG</td>\n",
       "      <td>AFG.1_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG.1.5_1</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Badakhshan</td>\n",
       "      <td>Jurm</td>\n",
       "      <td>AFG</td>\n",
       "      <td>AFG.1_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       GID_2       NAME_0      NAME_1     NAME_2 GID_0    GID_1\n",
       "0  AFG.1.1_1  Afghanistan  Badakhshan    Baharak   AFG  AFG.1_1\n",
       "1  AFG.1.2_1  Afghanistan  Badakhshan     Darwaz   AFG  AFG.1_1\n",
       "2  AFG.1.3_1  Afghanistan  Badakhshan   Fayzabad   AFG  AFG.1_1\n",
       "3  AFG.1.4_1  Afghanistan  Badakhshan  Ishkashim   AFG  AFG.1_1\n",
       "4  AFG.1.5_1  Afghanistan  Badakhshan       Jurm   AFG  AFG.1_1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Missing count su colonne comuni ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NAME_0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAME_2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID_0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID_1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GID_2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        missing\n",
       "NAME_0        0\n",
       "NAME_1        0\n",
       "NAME_2        0\n",
       "GID_0         0\n",
       "GID_1         0\n",
       "GID_2         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target codes selezionati: 7,936 (tipi: ['COUNTY', 'GADM1', 'GADM2', 'NUTS3'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping field geo_point_2d: unsupported OGR type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GADM2] Caricate 47,218 unità ADM2 uniche da data/gadm_410.gpkg\n",
      "NUTS pts: 1522 — US counties pts: 3233 — GADM1 pts: 3662 — GADM2 pts: 47218\n",
      "NUTS3 selezionati: 1522\n",
      "GADM2 selezionati: 82\n",
      "GADM1 selezionati (prima del filtro): 1738\n",
      "COUNTY selezionati: 3225\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodeID</th>\n",
       "      <th>nodeLabel</th>\n",
       "      <th>nodeName</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>DatasetDiOrigine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AGO.10_1</td>\n",
       "      <td>Huíla</td>\n",
       "      <td>-13.869244</td>\n",
       "      <td>15.319441</td>\n",
       "      <td>GADM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AGO.11_1</td>\n",
       "      <td>Luanda</td>\n",
       "      <td>-8.789466</td>\n",
       "      <td>13.385529</td>\n",
       "      <td>GADM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AGO.12_1</td>\n",
       "      <td>Lunda Norte</td>\n",
       "      <td>-8.303799</td>\n",
       "      <td>21.399342</td>\n",
       "      <td>GADM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AGO.13_1</td>\n",
       "      <td>Lunda Sul</td>\n",
       "      <td>-10.888833</td>\n",
       "      <td>19.163756</td>\n",
       "      <td>GADM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>AGO.14_1</td>\n",
       "      <td>Malanje</td>\n",
       "      <td>-9.323313</td>\n",
       "      <td>15.771779</td>\n",
       "      <td>GADM1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nodeID nodeLabel     nodeName   latitude  longitude DatasetDiOrigine\n",
       "0       1  AGO.10_1        Huíla -13.869244  15.319441            GADM1\n",
       "1       2  AGO.11_1       Luanda  -8.789466  13.385529            GADM1\n",
       "2       3  AGO.12_1  Lunda Norte  -8.303799  21.399342            GADM1\n",
       "3       4  AGO.13_1    Lunda Sul -10.888833  19.163756            GADM1\n",
       "4       5  AGO.14_1      Malanje  -9.323313  15.771779            GADM1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale nodi: 6,567\n",
      "✅ Salvato: node_list.csv (colonne: ['nodeID', 'nodeLabel', 'nodeName', 'latitude', 'longitude', 'DatasetDiOrigine'])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, Dict, Optional, Tuple, List, Set\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from IPython.display import display\n",
    "\n",
    "# === Percorsi (modifica se necessario) ===\n",
    "TSV_PATH = \"data/gadm1_nuts3_counties-gadm1_nuts3_counties - FB Social Connectedness Index - October 2021.tsv\"\n",
    "MAP_PATH = \"data/gadm1_nuts3_counties_levels.csv\"\n",
    "\n",
    "# Geodati\n",
    "NUTS_GEOJSON_PATH   = \"data/NUTS_RG_60M_2016_4326_LEVL_3.geojson\"\n",
    "US_COUNTIES_PATH    = \"data/us-county-boundaries.geojson\"\n",
    "GADM_GPKG_PATH      = \"data/gadm_410.gpkg\"   # layer tipico: \"gadm_410\"\n",
    "\n",
    "# Parametri lettura SCI\n",
    "SCI_COLS  = [\"user_loc\", \"fr_loc\", \"scaled_sci\"]\n",
    "DTYPE_MAP = {\"user_loc\": \"string\", \"fr_loc\": \"string\", \"scaled_sci\": \"float64\"}\n",
    "\n",
    "# ==========================\n",
    "# Utilità base\n",
    "# ==========================\n",
    "\n",
    "def _normalize_location_code(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalizza i codici location: stringa, strip, upper (non tocca NaN).\n",
    "    \"\"\"\n",
    "    return s.astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, required: Iterable[str], where: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Verifica che il DataFrame contenga le colonne richieste.\n",
    "    \"\"\"\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        loc = f\" in {where}\" if where else \"\"\n",
    "        raise ValueError(f\"Mancano colonne{loc}: {missing}\")\n",
    "\n",
    "\n",
    "def _preview(df: pd.DataFrame, name: str = \"DataFrame\", n: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Stampa anteprima, schema e numero righe.\n",
    "    \"\"\"\n",
    "    print(f\"== {name} – prime righe ==\")\n",
    "    display(df.head(n))\n",
    "    print(\"\\nSchema:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nNumero righe: {len(df):,}\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Lettura input SCI + mapping\n",
    "# ==========================\n",
    "\n",
    "def load_sci_tsv(\n",
    "    path: str,\n",
    "    sci_cols: Iterable[str] = (\"user_loc\", \"fr_loc\", \"scaled_sci\"),\n",
    "    dtype_map: Optional[Dict[str, str]] = None,\n",
    "    low_memory: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if dtype_map is None:\n",
    "        dtype_map = {\"user_loc\": \"string\", \"fr_loc\": \"string\", \"scaled_sci\": \"float64\"}\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        usecols=list(sci_cols),\n",
    "        dtype=dtype_map,\n",
    "        low_memory=low_memory\n",
    "    )\n",
    "\n",
    "    # normalizza codici\n",
    "    if \"user_loc\" in df.columns:\n",
    "        df[\"user_loc\"] = _normalize_location_code(df[\"user_loc\"])\n",
    "    if \"fr_loc\" in df.columns:\n",
    "        df[\"fr_loc\"] = _normalize_location_code(df[\"fr_loc\"])\n",
    "\n",
    "    _ensure_columns(df, sci_cols, where=\"SCI TSV\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_levels_mapping(\n",
    "    path: str,\n",
    "    usecols: Iterable[str] = (\"key\", \"level\"),\n",
    "    rename_map: Dict[str, str] = {\"key\": \"location_code\", \"level\": \"level_type\"},\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=list(usecols), dtype=\"string\").rename(columns=rename_map)\n",
    "    _ensure_columns(df, [\"location_code\", \"level_type\"], where=\"Mapping livelli\")\n",
    "    df[\"location_code\"] = _normalize_location_code(df[\"location_code\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def quick_read_inputs(\n",
    "    tsv_path: str,\n",
    "    map_path: str,\n",
    "    sci_cols: Iterable[str] = (\"user_loc\", \"fr_loc\", \"scaled_sci\"),\n",
    "    dtype_map: Optional[Dict[str, str]] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_sci = load_sci_tsv(tsv_path, sci_cols=sci_cols, dtype_map=dtype_map)\n",
    "    _preview(df_sci, name=\"SCI (TSV)\")\n",
    "    df_map = load_levels_mapping(map_path)\n",
    "    _preview(df_map, name=\"Mapping livelli\")\n",
    "    return df_sci, df_map\n",
    "\n",
    "# Esecuzione lettura\n",
    "df_sci, df_map = quick_read_inputs(TSV_PATH, MAP_PATH, sci_cols=SCI_COLS, dtype_map=DTYPE_MAP)\n",
    "\n",
    "# Facoltativo: distribuzione tipi livello\n",
    "print(\"\\n== level_type value_counts ==\")\n",
    "if \"level_type\" in df_map:\n",
    "    display(df_map['level_type'].value_counts())\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Metriche di copertura\n",
    "# ==========================\n",
    "\n",
    "def get_unique_location_codes(df_sci: pd.DataFrame) -> pd.Series:\n",
    "    _ensure_columns(df_sci, [\"user_loc\", \"fr_loc\"], where=\"SCI\")\n",
    "    u = _normalize_location_code(df_sci[\"user_loc\"]) \n",
    "    v = _normalize_location_code(df_sci[\"fr_loc\"]) \n",
    "    return pd.Index(u).append(pd.Index(v)).astype(\"string\").unique()\n",
    "\n",
    "\n",
    "def get_mapped_codes(df_map: pd.DataFrame) -> pd.Series:\n",
    "    _ensure_columns(df_map, [\"location_code\"], where=\"Mapping livelli\")\n",
    "    return _normalize_location_code(df_map[\"location_code\"]).dropna().unique()\n",
    "\n",
    "\n",
    "def compute_node_coverage(df_sci: pd.DataFrame, df_map: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    sci_codes = pd.Series(get_unique_location_codes(df_sci), name=\"location_code\")\n",
    "    map_codes = pd.Series(get_mapped_codes(df_map), name=\"location_code\")\n",
    "\n",
    "    total_codes = sci_codes.size\n",
    "    mapped_mask = sci_codes.isin(set(map_codes))\n",
    "    mapped_count = int(mapped_mask.sum())\n",
    "    unmapped = (\n",
    "        sci_codes[~mapped_mask].to_frame().drop_duplicates()\n",
    "        .sort_values(\"location_code\").reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    summary = {\n",
    "        \"total_unique_codes\": int(total_codes),\n",
    "        \"mapped_unique_codes\": int(mapped_count),\n",
    "        \"unmapped_unique_codes\": int(total_codes - mapped_count),\n",
    "        \"node_coverage_pct\": (mapped_count / total_codes * 100.0) if total_codes else 0.0,\n",
    "    }\n",
    "    return unmapped, summary\n",
    "\n",
    "\n",
    "def compute_edge_coverage(df_sci: pd.DataFrame, df_map: pd.DataFrame) -> Dict[str, float]:\n",
    "    _ensure_columns(df_sci, [\"user_loc\", \"fr_loc\"], where=\"SCI\")\n",
    "    mapped_set = set(get_mapped_codes(df_map))\n",
    "\n",
    "    u = _normalize_location_code(df_sci[\"user_loc\"]) \n",
    "    v = _normalize_location_code(df_sci[\"fr_loc\"]) \n",
    "\n",
    "    both_mapped_mask = u.isin(mapped_set) & v.isin(mapped_set)\n",
    "    total_rows = int(len(df_sci))\n",
    "    valid_rows = int(both_mapped_mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"valid_rows_both_mapped\": valid_rows,\n",
    "        \"edge_coverage_pct\": (valid_rows / total_rows * 100.0) if total_rows else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_country_coverage(\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    iso_col: str = \"country_ISO3\",\n",
    "    keep_only_intra_country: bool = True\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    if iso_col not in df_map.columns:\n",
    "        print(f\"[AVVISO] Colonna '{iso_col}' assente nel mapping: copertura per paese non calcolata.\")\n",
    "        return None\n",
    "\n",
    "    _ensure_columns(df_sci, [\"user_loc\", \"fr_loc\"], where=\"SCI\")\n",
    "\n",
    "    df_map_local = df_map[[\"location_code\", iso_col]].copy()\n",
    "    df_map_local[\"location_code\"] = _normalize_location_code(df_map_local[\"location_code\"])\n",
    "    code2iso = (\n",
    "        df_map_local.dropna().drop_duplicates(\"location_code\")\n",
    "        .set_index(\"location_code\")[iso_col]\n",
    "    )\n",
    "\n",
    "    sci = df_sci[[\"user_loc\", \"fr_loc\"]].copy()\n",
    "    sci[\"user_loc\"] = _normalize_location_code(sci[\"user_loc\"])\n",
    "    sci[\"fr_loc\"]   = _normalize_location_code(sci[\"fr_loc\"])\n",
    "\n",
    "    sci[\"iso_from\"] = sci[\"user_loc\"].map(code2iso)\n",
    "    sci[\"iso_to\"]   = sci[\"fr_loc\"].map(code2iso)\n",
    "\n",
    "    sci_valid = sci.dropna(subset=[\"iso_from\", \"iso_to\"]).copy()\n",
    "    if keep_only_intra_country:\n",
    "        sci_valid = sci_valid[sci_valid[\"iso_from\"] == sci_valid[\"iso_to\"]].copy()\n",
    "\n",
    "    edges_by_country = (\n",
    "        sci_valid.groupby(\"iso_from\", as_index=False)\n",
    "        .agg(edges=(\"user_loc\", \"size\")).rename(columns={\"iso_from\": iso_col})\n",
    "    )\n",
    "\n",
    "    nodes_from = sci_valid[[\"iso_from\", \"user_loc\"]].rename(columns={\"iso_from\": iso_col, \"user_loc\": \"loc\"})\n",
    "    nodes_to   = sci_valid[[\"iso_to\", \"fr_loc\"]].rename(columns={\"iso_to\": iso_col, \"fr_loc\": \"loc\"})\n",
    "    nodes_all  = pd.concat([nodes_from, nodes_to], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    nodes_by_country = nodes_all.groupby(iso_col, as_index=False).agg(nodes=(\"loc\", \"nunique\"))\n",
    "\n",
    "    country_cov = edges_by_country.merge(nodes_by_country, on=iso_col, how=\"outer\").fillna(0)\n",
    "    country_cov[\"edges\"] = country_cov[\"edges\"].astype(int)\n",
    "    country_cov[\"nodes\"] = country_cov[\"nodes\"].astype(int)\n",
    "\n",
    "    total_edges = int(country_cov[\"edges\"].sum()) if len(country_cov) else 0\n",
    "    country_cov[\"edges_pct\"] = (100.0 * country_cov[\"edges\"] / total_edges) if total_edges else 0.0\n",
    "\n",
    "    return country_cov.sort_values([\"edges\", \"nodes\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def coverage_report(df_sci: pd.DataFrame, df_map: pd.DataFrame, top_n: int = 10) -> None:\n",
    "    unmapped_nodes, node_summary = compute_node_coverage(df_sci, df_map)\n",
    "    print(\"== Copertura NODI ==\")\n",
    "    for k, v in node_summary.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "    if len(unmapped_nodes) > 0:\n",
    "        print(f\"\\nEsempi codici NON mappati ({min(10, len(unmapped_nodes))}):\")\n",
    "        display(unmapped_nodes.head(10))\n",
    "\n",
    "    edge_summary = compute_edge_coverage(df_sci, df_map)\n",
    "    print(\"\\n== Copertura ARCHI ==\")\n",
    "    for k, v in edge_summary.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "    country_cov = compute_country_coverage(df_sci, df_map)\n",
    "    if country_cov is not None and len(country_cov):\n",
    "        print(\"\\n== Copertura PER PAESE (prime righe) ==\")\n",
    "        display(country_cov.head(top_n))\n",
    "    else:\n",
    "        print(\"\\n== Copertura PER PAESE ==\")\n",
    "        print(\"Mapping senza 'country_ISO3' → se vuoi questa sezione, aggiungi la colonna a df_map.\")\n",
    "\n",
    "# Esecuzione report\n",
    "coverage_report(df_sci, df_map)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Funzioni geodati (con 'name' e dataset di origine)\n",
    "# ==========================\n",
    "\n",
    "def _representative_points(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].representative_point()\n",
    "    gdf[\"latitude\"] = gdf.geometry.y\n",
    "    gdf[\"longitude\"] = gdf.geometry.x\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def load_nuts3_points(nuts_geojson_path: str, code_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(nuts_geojson_path)\n",
    "    # Filtro LEVL=3\n",
    "    levl_col = next((c for c in [\"LEVL_CODE\", \"LEVL\", \"LEVEL\"] if c in gdf.columns), None)\n",
    "    if levl_col is not None:\n",
    "        gdf = gdf[gdf[levl_col].astype(str).isin([\"3\", 3])].copy()\n",
    "\n",
    "    # Codice e Nome\n",
    "    candidates_code = [code_col] if code_col else [c for c in [\"NUTS_ID\", \"nuts_id\", \"ID\", \"id\"] if c in gdf.columns]\n",
    "    if not candidates_code:\n",
    "        raise ValueError(\"Non trovo una colonna codice per NUTS (es. 'NUTS_ID'). Passa code_col=...\")\n",
    "    cc = candidates_code[0]\n",
    "\n",
    "    name_col = next((c for c in [\"NUTS_NAME\", \"NAME_LATN\", \"NAME_ENGL\", \"NAME\"] if c in gdf.columns), None)\n",
    "\n",
    "    gdf = gdf.rename(columns={cc: \"code\", (name_col if name_col else cc): \"name\"})\n",
    "    cols = [\"code\", \"name\", \"geometry\"] if name_col else [\"code\", \"geometry\"]\n",
    "    gdf = _representative_points(gdf[cols].dropna(subset=[\"code\"]))\n",
    "\n",
    "    gdf[\"code\"] = _normalize_location_code(gdf[\"code\"])  # es. \"AL034\"\n",
    "    if \"name\" not in gdf:\n",
    "        gdf[\"name\"] = pd.NA\n",
    "\n",
    "    out = gdf[[\"code\", \"name\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_gadm2_points(gadm_gpkg_path: str, layer: str = \"gadm_410\", code_col: str = \"GID_2\") -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(gadm_gpkg_path, layer=layer)\n",
    "    if code_col not in gdf.columns:\n",
    "        raise ValueError(f\"La colonna {code_col!r} non esiste nel layer {layer}. Colonne trovate: {list(gdf.columns)}\")\n",
    "\n",
    "    name_col = \"NAME_2\" if \"NAME_2\" in gdf.columns else None\n",
    "\n",
    "    gdf2 = gdf[~gdf[code_col].isna()].copy().rename(columns={code_col: \"code\"})\n",
    "    if name_col:\n",
    "        gdf2 = gdf2.rename(columns={name_col: \"name\"})\n",
    "    gdf2 = gdf2.to_crs(4326)\n",
    "    gdf2[\"geometry\"] = gdf2.geometry.representative_point()\n",
    "    gdf2[\"latitude\"] = gdf2.geometry.y\n",
    "    gdf2[\"longitude\"] = gdf2.geometry.x\n",
    "    gdf2[\"code\"] = gdf2[\"code\"].astype(\"string\").str.strip().str.upper()  # es. \"AGO.4.7_1\"\n",
    "    if \"name\" not in gdf2:\n",
    "        gdf2[\"name\"] = pd.NA\n",
    "\n",
    "    out = gdf2[[\"code\", \"name\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "    print(f\"[GADM2] Caricate {len(out):,} unità ADM2 uniche da {gadm_gpkg_path}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_us_counties_points(counties_geojson_path: str, code_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(counties_geojson_path)\n",
    "    candidates = [code_col] if code_col else [c for c in [\"GEOID\", \"geoid\", \"FIPS\", \"fips\"] if c in gdf.columns]\n",
    "    if not candidates:\n",
    "        raise ValueError(\"Non trovo una colonna codice per US counties (es. 'GEOID'). Passa code_col=...\")\n",
    "    cc = candidates[0]\n",
    "\n",
    "    name_col = next((c for c in [\"NAME\", \"NAMELSAD\", \"name\"] if c in gdf.columns), None)\n",
    "\n",
    "    gdf = gdf.rename(columns={cc: \"code\", (name_col if name_col else cc): \"name\"})\n",
    "    gdf[\"code\"] = gdf[\"code\"].astype(str).str.zfill(5)  # GEOID a 5 cifre (es. 06091)\n",
    "    gdf = _representative_points(gdf[[\"code\", \"name\", \"geometry\"]].dropna(subset=[\"code\"]))\n",
    "    gdf[\"code\"] = _normalize_location_code(gdf[\"code\"])  # manteniamo 5 cifre\n",
    "\n",
    "    out = gdf[[\"code\", \"name\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_gadm1_points(gadm_gpkg_path: str, layer: str = \"gadm_410\", code_col: str = \"GID_1\") -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(gadm_gpkg_path, layer=layer)\n",
    "    if code_col not in gdf.columns:\n",
    "        raise ValueError(f\"Colonna {code_col!r} assente in {layer}. Colonne: {list(gdf.columns)}\")\n",
    "    name_col = \"NAME_1\" if \"NAME_1\" in gdf.columns else None\n",
    "\n",
    "    gdf1 = gdf[~gdf[code_col].isna()].copy().rename(columns={code_col: \"code\"})\n",
    "    if name_col:\n",
    "        gdf1 = gdf1.rename(columns={name_col: \"name\"})\n",
    "    gdf1 = gdf1.to_crs(4326)\n",
    "    gdf1[\"geometry\"] = gdf1.geometry.representative_point()\n",
    "    gdf1[\"latitude\"] = gdf1.geometry.y\n",
    "    gdf1[\"longitude\"] = gdf1.geometry.x\n",
    "    gdf1[\"code\"] = gdf1[\"code\"].astype(\"string\").str.strip().str.upper()  # es. \"AGO.4_1\"\n",
    "    if \"name\" not in gdf1:\n",
    "        gdf1[\"name\"] = pd.NA\n",
    "\n",
    "    return gdf1[[\"code\", \"name\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Selezione target dal mapping (con GADM2)\n",
    "# ==========================\n",
    "\n",
    "def select_target_codes(\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    level_types: Iterable[str] = (\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\"),\n",
    "    sci_required: Iterable[str] = (\"user_loc\", \"fr_loc\"),\n",
    "    map_required: Iterable[str] = (\"location_code\", \"level_type\"),\n",
    ") -> pd.DataFrame:\n",
    "    _ensure_columns(df_sci, sci_required, \"SCI\")\n",
    "    _ensure_columns(df_map, map_required, \"Mapping\")\n",
    "\n",
    "    # Codici presenti nello SCI (user_loc ∪ fr_loc)\n",
    "    sci_codes = pd.Index(_normalize_location_code(df_sci[\"user_loc\"])) \\\n",
    "                  .append(pd.Index(_normalize_location_code(df_sci[\"fr_loc\"]))) \\\n",
    "                  .unique()\n",
    "\n",
    "    # Filtra mapping per i tipi desiderati e normalizza\n",
    "    df_map2 = df_map.copy()\n",
    "    df_map2[\"location_code\"] = _normalize_location_code(df_map2[\"location_code\"])\n",
    "    df_map2[\"level_type\"] = df_map2[\"level_type\"].astype(\"string\")\n",
    "\n",
    "    wanted = {t.upper() for t in level_types}\n",
    "    df_map2 = df_map2[df_map2[\"level_type\"].str.upper().isin(wanted)]\n",
    "\n",
    "    # Intersezione SCI ∩ mapping e deduplica\n",
    "    target = (df_map2[df_map2[\"location_code\"].isin(set(sci_codes))]\n",
    "              .drop_duplicates(subset=[\"location_code\", \"level_type\"])  # possono esistere più tipi\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    print(f\"[INFO] Target codes selezionati: {len(target):,} \"\n",
    "          f\"(tipi: {sorted(target['level_type'].str.upper().unique())})\")\n",
    "    return target\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Normalizzatori dalle chiavi di mapping -> chiavi geodati\n",
    "# ==========================\n",
    "\n",
    "def normalize_gadm1_mapping_code(code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Esempi attesi nel mapping: 'BGD1', 'IND23', ...\n",
    "    Output (GADM v4): 'ISO3.ADM1_1'  es. 'BGD.1_1'\n",
    "    \"\"\"\n",
    "    if code is None or pd.isna(code):\n",
    "        return None\n",
    "    s = str(code).strip().upper()\n",
    "    # Se già in formato GID_1\n",
    "    if re.fullmatch(r\"[A-Z]{3}\\.\\d+_1\", s):\n",
    "        return s\n",
    "    m = re.fullmatch(r'([A-Z]{3})(\\d+)', s)\n",
    "    if not m:\n",
    "        return None\n",
    "    iso3, adm1 = m.groups()\n",
    "    return f\"{iso3}.{int(adm1)}_1\"\n",
    "\n",
    "\n",
    "def normalize_gadm2_mapping_code(code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Converte varie forme in GADM v4 GID_2: 'ISO3.A.B_1' (es. 'AGO.4.7_1').\n",
    "    Accetta anche forme come 'AGO4-7', 'AGO4.7', 'AGO-4-7', 'AGO4_7', ecc.\n",
    "    \"\"\"\n",
    "    if code is None or pd.isna(code):\n",
    "        return None\n",
    "    s = str(code).strip().upper()\n",
    "    # Già in formato GID_2\n",
    "    if re.fullmatch(r\"[A-Z]{3}\\.\\d+\\.\\d+_1\", s):\n",
    "        return s\n",
    "    # Pattern flessibile: ISO3 + sep + ADM1 + sep + ADM2 (sep = . - _ o nessuno tra num)\n",
    "    m = re.fullmatch(r\"([A-Z]{3})[\\.-_ ]?(\\d+)[\\.-_ ]?(\\d+)\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    iso3, adm1, adm2 = m.groups()\n",
    "    return f\"{iso3}.{int(adm1)}.{int(adm2)}_1\"\n",
    "\n",
    "\n",
    "def normalize_county_mapping_code(code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    'USA06091' -> '06091' (GEOID a 5 cifre) per il join; il nodeLabel verrà poi ricostruito come 'USA' + GEOID.\n",
    "    \"\"\"\n",
    "    if code is None or pd.isna(code):\n",
    "        return None\n",
    "    s = str(code).strip().upper()\n",
    "    if s.startswith(\"USA\"):\n",
    "        s = s[3:]\n",
    "    s = re.sub(r'\\D', '', s)\n",
    "    if len(s) == 5:\n",
    "        return s\n",
    "    if 1 <= len(s) <= 5:\n",
    "        return s.zfill(5)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Costruzione nodi per tipo (con name + DatasetDiOrigine + nodeLabel coerente)\n",
    "# ==========================\n",
    "\n",
    "def build_nodes_for_type_with_transform(\n",
    "    target_codes: pd.DataFrame,\n",
    "    type_name: str,\n",
    "    source_df_points: pd.DataFrame,\n",
    "    transform_fn=None\n",
    ") -> pd.DataFrame:\n",
    "    mask = target_codes[\"level_type\"].str.upper() == type_name.upper()\n",
    "    wanted = target_codes.loc[mask, [\"location_code\"]].copy()\n",
    "    wanted[\"location_code\"] = wanted[\"location_code\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "    # Chiave 'code' per il join con i geodati\n",
    "    if transform_fn is not None:\n",
    "        wanted[\"code\"] = wanted[\"location_code\"].map(transform_fn)\n",
    "    else:\n",
    "        # Se il mapping già usa la stessa codifica dei geodati\n",
    "        wanted[\"code\"] = wanted[\"location_code\"]\n",
    "\n",
    "    wanted = wanted.dropna(subset=[\"code\"]).drop_duplicates(\"code\")\n",
    "\n",
    "    pts = source_df_points.copy()\n",
    "    pts[\"code\"] = pts[\"code\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "    out = wanted.merge(pts, on=\"code\", how=\"inner\")\n",
    "\n",
    "    # nodeLabel desiderato per tipo\n",
    "    type_upper = type_name.upper()\n",
    "    if type_upper == \"COUNTY\":\n",
    "        # Mostra 'USA' + GEOID a 5 cifre (es. USA06091)\n",
    "        out[\"nodeLabel\"] = \"USA\" + out[\"code\"].astype(str).str.zfill(5)\n",
    "    else:\n",
    "        # Per NUTS3/GADM1/GADM2 la 'code' è già AL034 / AGO.4_1 / AGO.4.7_1\n",
    "        out[\"nodeLabel\"] = out[\"code\"]\n",
    "\n",
    "    out[\"DatasetDiOrigine\"] = type_upper\n",
    "    # Rinominare 'name' -> 'nodeName' se presente\n",
    "    if \"name\" in out.columns:\n",
    "        out = out.rename(columns={\"name\": \"nodeName\"})\n",
    "    else:\n",
    "        out[\"nodeName\"] = pd.NA\n",
    "\n",
    "    cols = [\"nodeLabel\", \"nodeName\", \"latitude\", \"longitude\", \"DatasetDiOrigine\"]\n",
    "    return out[cols].drop_duplicates(\"nodeLabel\")\n",
    "\n",
    "\n",
    "def build_nodes_for_type(\n",
    "    target_codes: pd.DataFrame,\n",
    "    type_name: str,\n",
    "    source_df_points: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    return build_nodes_for_type_with_transform(\n",
    "        target_codes=target_codes,\n",
    "        type_name=type_name,\n",
    "        source_df_points=source_df_points,\n",
    "        transform_fn=None\n",
    "    )\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Assemblaggio finale con priorità GADM2 > GADM1\n",
    "# ==========================\n",
    "\n",
    "def _gid1_base_from_gid1(gid1: str) -> Optional[str]:\n",
    "    # 'AGO.4_1' -> 'AGO.4'\n",
    "    if not isinstance(gid1, str):\n",
    "        return None\n",
    "    s = gid1.strip().upper()\n",
    "    m = re.fullmatch(r\"([A-Z]{3}\\.\\d+)_1\", s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def _gid1_base_from_gid2(gid2: str) -> Optional[str]:\n",
    "    # 'AGO.4.7_1' -> 'AGO.4'\n",
    "    if not isinstance(gid2, str):\n",
    "        return None\n",
    "    s = gid2.strip().upper()\n",
    "    m = re.fullmatch(r\"([A-Z]{3}\\.\\d+)\\.\\d+_1\", s)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def drop_gadm1_if_gadm2_present(gadm1_nodes: pd.DataFrame, gadm2_nodes: pd.DataFrame) -> pd.DataFrame:\n",
    "    if gadm1_nodes is None or len(gadm1_nodes) == 0:\n",
    "        return gadm1_nodes\n",
    "    if gadm2_nodes is None or len(gadm2_nodes) == 0:\n",
    "        return gadm1_nodes\n",
    "\n",
    "    gadm1 = gadm1_nodes.copy()\n",
    "    gadm2 = gadm2_nodes.copy()\n",
    "\n",
    "    gadm1[\"_gid1base\"] = gadm1[\"nodeLabel\"].map(_gid1_base_from_gid1)\n",
    "    gadm2_bases: Set[str] = set(gadm2[\"nodeLabel\"].map(_gid1_base_from_gid2).dropna().unique())\n",
    "\n",
    "    # Mantieni solo GADM1 che NON hanno un GADM2 nello stesso ADM1\n",
    "    keep_mask = ~gadm1[\"_gid1base\"].isin(gadm2_bases)\n",
    "    kept = gadm1.loc[keep_mask, [c for c in gadm1.columns if c != \"_gid1base\"]].copy()\n",
    "    dropped = len(gadm1) - len(kept)\n",
    "    if dropped:\n",
    "        print(f\"[INFO] Rimossi {dropped:,} nodi GADM1 perché coperti da GADM2 nello stesso ADM1\")\n",
    "    return kept\n",
    "\n",
    "\n",
    "def assemble_node_list(\n",
    "    nuts_nodes: Optional[pd.DataFrame] = None,\n",
    "    gadm2_nodes: Optional[pd.DataFrame] = None,\n",
    "    gadm1_nodes: Optional[pd.DataFrame] = None,\n",
    "    county_nodes: Optional[pd.DataFrame] = None\n",
    ") -> pd.DataFrame:\n",
    "    # Applica priorità: NUTS3 (UE) + GADM2 + (GADM1 senza duplicati ADM1) + COUNTIES\n",
    "    if gadm1_nodes is not None and gadm2_nodes is not None:\n",
    "        gadm1_nodes = drop_gadm1_if_gadm2_present(gadm1_nodes, gadm2_nodes)\n",
    "\n",
    "    frames = [df for df in [nuts_nodes, gadm2_nodes, gadm1_nodes, county_nodes] if df is not None and len(df)]\n",
    "    if not frames:\n",
    "        raise ValueError(\"Nessun nodo fornito.\")\n",
    "\n",
    "    nodes = pd.concat(frames, ignore_index=True)\n",
    "    nodes = nodes.drop_duplicates(\"nodeLabel\")\n",
    "    nodes = nodes.sort_values(\"nodeLabel\").reset_index(drop=True)\n",
    "    nodes.insert(0, \"nodeID\", range(1, len(nodes) + 1))\n",
    "\n",
    "    # Output finale richiesto: nodeID, nodeLabel, nodeName, latitude, longitude, DatasetDiOrigine\n",
    "    return nodes[[\"nodeID\", \"nodeLabel\", \"nodeName\", \"latitude\", \"longitude\", \"DatasetDiOrigine\"]]\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Ispezioni rapide dei layer (facoltative)\n",
    "# ==========================\n",
    "layers = fiona.listlayers(GADM_GPKG_PATH)\n",
    "print(f\"Layer trovati in {GADM_GPKG_PATH}:\")\n",
    "for i, lyr in enumerate(layers, 1):\n",
    "    print(f\"{i:>2}. {lyr}\")\n",
    "\n",
    "# Anteprime\n",
    "try:\n",
    "    nuts_sample = gpd.read_file(NUTS_GEOJSON_PATH, rows=5)\n",
    "except TypeError:\n",
    "    nuts_sample = gpd.read_file(NUTS_GEOJSON_PATH)\n",
    "print(\"\\nColonne disponibili in NUTS3 GeoJSON:\")\n",
    "print(list(nuts_sample.columns))\n",
    "try:\n",
    "    from IPython.display import display as _display\n",
    "    _display(nuts_sample.head())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    counties_sample = gpd.read_file(US_COUNTIES_PATH, rows=5)\n",
    "except TypeError:\n",
    "    counties_sample = gpd.read_file(US_COUNTIES_PATH)\n",
    "print(\"\\nColonne disponibili in US counties GeoJSON:\")\n",
    "print(list(counties_sample.columns))\n",
    "try:\n",
    "    _display(counties_sample.head())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Diagnostica GADM (livello 2)\n",
    "def inspect_gadm2(gpkg_path: str, layer: str = \"gadm_410\", prefer_code_cols=(\"GID_2\",\"ID_2\",\"GID2\")):\n",
    "    gdf = gpd.read_file(gpkg_path, layer=layer)\n",
    "    print(f\"[INFO] Caricato layer='{layer}' con {len(gdf):,} feature\")\n",
    "    print(f\"[INFO] CRS: {gdf.crs}\")\n",
    "    print(\"[INFO] Colonne disponibili:\")\n",
    "    print(list(gdf.columns))\n",
    "\n",
    "    cols_up = {c.upper(): c for c in gdf.columns}\n",
    "    code_col = None\n",
    "    for pref in prefer_code_cols:\n",
    "        if pref in cols_up:\n",
    "            code_col = cols_up[pref]\n",
    "            break\n",
    "    if code_col is None:\n",
    "        candidates = [orig for up, orig in cols_up.items() if up.endswith(\"_2\") or \"GID\" in up]\n",
    "        if not candidates:\n",
    "            raise ValueError(\"Non trovo una colonna codice per ADM2 (tipo 'GID_2' o 'ID_2').\")\n",
    "        code_col = candidates[0]\n",
    "\n",
    "    print(f\"[INFO] Colonna codice ADM2 individuata: {code_col!r}\")\n",
    "    gdf_adm2 = gdf[~gdf[code_col].isna()].copy()\n",
    "    print(f\"[INFO] Righe ADM2 (non-NaN su {code_col}): {len(gdf_adm2):,}\")\n",
    "\n",
    "    unique_codes = gdf_adm2[code_col].astype(\"string\").str.strip().str.upper().nunique()\n",
    "    print(f\"[INFO] Codici ADM2 unici: {unique_codes:,}\")\n",
    "\n",
    "    geom_types = gdf_adm2.geom_type.value_counts().to_dict()\n",
    "    print(f\"[INFO] Geometrie (conteggio per tipo): {geom_types}\")\n",
    "\n",
    "    candidates_name = [c for c in [\"NAME_0\",\"NAME_1\",\"NAME_2\",\"GID_0\",\"GID_1\",\"GID_2\"] if c in gdf_adm2.columns]\n",
    "    show_cols = [code_col] + candidates_name\n",
    "    show_cols = [c for c in dict.fromkeys(show_cols)]  # dedupe preservando ordine\n",
    "\n",
    "    print(\"\\n== Anteprima ADM2 (prime 5 righe) ==\")\n",
    "    try:\n",
    "        _display(gdf_adm2[show_cols].head())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if any(c in gdf_adm2.columns for c in [\"NAME_0\",\"NAME_1\",\"NAME_2\",\"GID_0\",\"GID_1\",\"GID_2\"]):\n",
    "        print(\"\\n== Missing count su colonne comuni ==\")\n",
    "        check_cols = [c for c in [\"NAME_0\",\"NAME_1\",\"NAME_2\",\"GID_0\",\"GID_1\",\"GID_2\"] if c in gdf_adm2.columns]\n",
    "        try:\n",
    "            _display(gdf_adm2[check_cols].isna().sum().to_frame(\"missing\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return gdf, gdf_adm2, code_col\n",
    "\n",
    "# Esecuzione diagnostica (facoltativa, utile la prima volta)\n",
    "gdf_full, gdf_adm2, gadm_code_col = inspect_gadm2(GADM_GPKG_PATH, layer=\"gadm_410\")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Pipeline NODI\n",
    "# ==========================\n",
    "\n",
    "# 0) Restrizione ai codici presenti in SCI + mapping (incluso GADM2)\n",
    "target = select_target_codes(\n",
    "    df_sci=df_sci,\n",
    "    df_map=df_map,\n",
    "    level_types=(\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\")\n",
    ")\n",
    "\n",
    "# 1) Carico punti per ciascun tipo (con 'name')\n",
    "nuts_pts    = load_nuts3_points(NUTS_GEOJSON_PATH)\n",
    "county_pts  = load_us_counties_points(US_COUNTIES_PATH)\n",
    "gadm1_pts   = load_gadm1_points(GADM_GPKG_PATH, layer=\"gadm_410\", code_col=\"GID_1\")\n",
    "gadm2_pts   = load_gadm2_points(GADM_GPKG_PATH, layer=\"gadm_410\", code_col=\"GID_2\")\n",
    "\n",
    "print(\"NUTS pts:\", len(nuts_pts), \"— US counties pts:\", len(county_pts), \"— GADM1 pts:\", len(gadm1_pts), \"— GADM2 pts:\", len(gadm2_pts))\n",
    "\n",
    "# 2) Join con eventuale trasformazione delle chiavi\n",
    "nuts_nodes    = build_nodes_for_type_with_transform(target, \"NUTS3\",  nuts_pts,   transform_fn=None)\n",
    "gadm2_nodes   = build_nodes_for_type_with_transform(target, \"GADM2\",  gadm2_pts,  transform_fn=normalize_gadm2_mapping_code)\n",
    "gadm1_nodes   = build_nodes_for_type_with_transform(target, \"GADM1\",  gadm1_pts,  transform_fn=normalize_gadm1_mapping_code)\n",
    "county_nodes  = build_nodes_for_type_with_transform(target, \"COUNTY\", county_pts, transform_fn=normalize_county_mapping_code)\n",
    "\n",
    "print(\"NUTS3 selezionati:\", len(nuts_nodes))\n",
    "print(\"GADM2 selezionati:\", len(gadm2_nodes))\n",
    "print(\"GADM1 selezionati (prima del filtro):\", len(gadm1_nodes))\n",
    "print(\"COUNTY selezionati:\", len(county_nodes))\n",
    "\n",
    "# 3) Assemblaggio finale con priorità GADM2 > GADM1\n",
    "node_list = assemble_node_list(nuts_nodes, gadm2_nodes, gadm1_nodes, county_nodes)\n",
    "\n",
    "try:\n",
    "    display(node_list.head())\n",
    "except Exception:\n",
    "    pass\n",
    "print(f\"Totale nodi: {len(node_list):,}\")\n",
    "\n",
    "# 4) Salvataggio\n",
    "OUTPUT_CSV = \"node_list.csv\"\n",
    "node_list.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Salvato: {OUTPUT_CSV} (colonne: {list(node_list.columns)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f935d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Copertura FB → NODI per TIPO ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>fb_unique_nodes</th>\n",
       "      <th>matched_nodes</th>\n",
       "      <th>coverage_pct</th>\n",
       "      <th>unmatched_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COUNTY</td>\n",
       "      <td>3225</td>\n",
       "      <td>3225</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GADM1</td>\n",
       "      <td>1819</td>\n",
       "      <td>1738</td>\n",
       "      <td>95.547004</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GADM2</td>\n",
       "      <td>1370</td>\n",
       "      <td>82</td>\n",
       "      <td>5.985401</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUTS3</td>\n",
       "      <td>1522</td>\n",
       "      <td>1522</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_type  fb_unique_nodes  matched_nodes  coverage_pct  unmatched_nodes\n",
       "0       COUNTY             3225           3225    100.000000                0\n",
       "1        GADM1             1819           1738     95.547004               81\n",
       "2        GADM2             1370             82      5.985401             1288\n",
       "3        NUTS3             1522           1522    100.000000                0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_fb_node_coverage(\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    node_list: pd.DataFrame,\n",
    "    allowed_datasets: Iterable[str] = (\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\"),\n",
    "    by_country: bool = True\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Report di copertura dei NODI del Facebook SCI (user_loc ∪ fr_loc) rispetto ai nodi costruiti.\n",
    "    Ritorna (df_type, df_country_or_None).\n",
    "    \"\"\"\n",
    "    # Helpers locali\n",
    "    def _detect_country_cols_local(df_map: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "        iso_candidates = [\"country_ISO3\", \"ISO3\", \"iso3\", \"COUNTRY_ISO3\", \"country_iso3\"]\n",
    "        name_candidates = [\"country_name\", \"COUNTRY_NAME\", \"country\", \"COUNTRY\", \"NAME_0\"]\n",
    "        iso_col = next((c for c in iso_candidates if c in df_map.columns), None)\n",
    "        name_col = next((c for c in name_candidates if c in df_map.columns), None)\n",
    "        return iso_col, name_col\n",
    "\n",
    "    def _node_label_from_mapping_row(level_type: str, location_code: str) -> Optional[str]:\n",
    "        lt = (level_type or \"\").upper()\n",
    "        if pd.isna(location_code):\n",
    "            return None\n",
    "        code = str(location_code).strip().upper()\n",
    "        if lt == \"NUTS3\":\n",
    "            return code\n",
    "        if lt == \"GADM1\":\n",
    "            return normalize_gadm1_mapping_code(code)\n",
    "        if lt == \"GADM2\":\n",
    "            return normalize_gadm2_mapping_code(code)\n",
    "        if lt == \"COUNTY\":\n",
    "            c = normalize_county_mapping_code(code)\n",
    "            return None if c is None else (\"USA\" + str(c).zfill(5))\n",
    "        return None\n",
    "\n",
    "    allowed = {t.upper() for t in allowed_datasets}\n",
    "\n",
    "    # 1) Codici FB unici (user_loc ∪ fr_loc)\n",
    "    sci_codes = set(get_unique_location_codes(df_sci))\n",
    "\n",
    "    # 2) Set di nodeLabel presenti per dataset nei nodi\n",
    "    labels_by_ds: dict[str, set[str]] = {\n",
    "        ds: set(node_list.loc[node_list[\"DatasetDiOrigine\"].str.upper() == ds, \"nodeLabel\"].astype(str))\n",
    "        for ds in [\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\"]\n",
    "    }\n",
    "\n",
    "    # 3) Colonne paese (se ci sono)\n",
    "    iso_col, name_col = _detect_country_cols_local(df_map)\n",
    "\n",
    "    rows_type = []\n",
    "    rows_country = []\n",
    "\n",
    "    for ds in [\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\"]:\n",
    "        if ds not in allowed:\n",
    "            continue\n",
    "\n",
    "        mm = df_map[df_map[\"level_type\"].str.upper() == ds].copy()\n",
    "        mm = mm[mm[\"location_code\"].isin(sci_codes)].copy()\n",
    "\n",
    "        mm[\"nodeLabel\"] = mm.apply(lambda r: _node_label_from_mapping_row(ds, r[\"location_code\"]), axis=1)\n",
    "        mm = mm.dropna(subset=[\"nodeLabel\"]).drop_duplicates(\"location_code\")\n",
    "\n",
    "        fb_unique = mm[\"location_code\"].nunique()\n",
    "        matched = int(mm[\"nodeLabel\"].isin(labels_by_ds.get(ds, set())).sum())\n",
    "\n",
    "        rows_type.append({\n",
    "            \"dataset_type\": ds,\n",
    "            \"fb_unique_nodes\": int(fb_unique),\n",
    "            \"matched_nodes\": matched,\n",
    "            \"coverage_pct\": (matched / fb_unique * 100.0) if fb_unique else 0.0,\n",
    "            \"unmatched_nodes\": int(fb_unique - matched),\n",
    "        })\n",
    "\n",
    "        if by_country and iso_col is not None:\n",
    "            cols = [\"location_code\", iso_col]\n",
    "            if name_col and name_col in df_map.columns and name_col != iso_col:\n",
    "                cols.append(name_col)\n",
    "            tmp = mm.merge(df_map[cols], on=\"location_code\", how=\"left\").rename(columns={iso_col: \"country_ISO3\"})\n",
    "            if name_col and name_col in tmp.columns and name_col != iso_col:\n",
    "                tmp = tmp.rename(columns={name_col: \"country_name\"})\n",
    "            else:\n",
    "                tmp[\"country_name\"] = pd.NA\n",
    "            tmp[\"is_matched\"] = tmp[\"nodeLabel\"].isin(labels_by_ds.get(ds, set()))\n",
    "\n",
    "            for (iso3, cname), grp in tmp.groupby([\"country_ISO3\", \"country_name\"], dropna=False):\n",
    "                fb_u = grp[\"location_code\"].nunique()\n",
    "                m_u = int(grp[\"is_matched\"].sum())\n",
    "                rows_country.append({\n",
    "                    \"country_ISO3\": iso3,\n",
    "                    \"country_name\": cname,\n",
    "                    \"dataset_type\": ds,\n",
    "                    \"fb_unique_nodes\": int(fb_u),\n",
    "                    \"matched_nodes\": m_u,\n",
    "                    \"coverage_pct\": (m_u / fb_u * 100.0) if fb_u else 0.0,\n",
    "                    \"unmatched_nodes\": int(fb_u - m_u),\n",
    "                })\n",
    "\n",
    "    df_type = pd.DataFrame(rows_type).sort_values([\"dataset_type\"]).reset_index(drop=True)\n",
    "    df_country = (pd.DataFrame(rows_country)\n",
    "                    .sort_values([\"dataset_type\", \"country_ISO3\"])\n",
    "                    .reset_index(drop=True)) if rows_country else None\n",
    "\n",
    "    print(\"\\n== Copertura FB → NODI per TIPO ==\")\n",
    "    try:\n",
    "        from IPython.display import display as _display\n",
    "        _display(df_type)\n",
    "    except Exception:\n",
    "        print(df_type)\n",
    "\n",
    "    if df_country is not None:\n",
    "        print(\"\\n== Copertura FB → NODI per PAESE × TIPO (prime 20 righe) ==\")\n",
    "        try:\n",
    "            _display(df_country.head(20))\n",
    "        except Exception:\n",
    "            print(df_country.head(20))\n",
    "\n",
    "    return df_type, df_country\n",
    "\n",
    "\n",
    "coverage_by_type, coverage_by_country = summarize_fb_node_coverage(\n",
    "    df_sci=df_sci,\n",
    "    df_map=df_map,\n",
    "    node_list=node_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ffd304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target codes selezionati: 7,936 (tipi: ['COUNTY', 'GADM1', 'GADM2', 'NUTS3'])\n",
      "✅ Salvato edges generale: ./edges_all.csv (righe: 43,125,489)\n",
      "✅ Salvati 145 file intra-paese in: ./edges_by_country\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# EDGE LIST: generale e per-paese (ISO3 auto da NUTS/GADM/USA)\n",
    "# ==========================\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Iterable, Dict, Optional, Tuple, List, Set\n",
    "\n",
    "# --- Mappa NUTS alpha-2 -> ISO3 ricavata dal GeoJSON NUTS ---\n",
    "def build_nuts_a2_to_iso3_map(nuts_geojson_path: str) -> Dict[str, str]:\n",
    "    gdf = gpd.read_file(nuts_geojson_path)\n",
    "    # CNTR_CODE è alpha-2 (con le eccezioni NUTS: UK, EL)\n",
    "    if \"CNTR_CODE\" not in gdf.columns:\n",
    "        raise ValueError(\"Nel GeoJSON NUTS manca la colonna 'CNTR_CODE'.\")\n",
    "    a2_vals = (\n",
    "        gdf[\"CNTR_CODE\"]\n",
    "        .dropna().astype(str).str.upper().unique().tolist()\n",
    "    )\n",
    "    mapping: Dict[str, str] = {}\n",
    "    try:\n",
    "        import pycountry\n",
    "        for a2 in a2_vals:\n",
    "            a2_norm = \"GB\" if a2 == \"UK\" else (\"GR\" if a2 == \"EL\" else a2)\n",
    "            c = pycountry.countries.get(alpha_2=a2_norm)\n",
    "            if c and hasattr(c, \"alpha_3\"):\n",
    "                mapping[a2] = c.alpha_3\n",
    "    except Exception:\n",
    "        # fallback minimo se pycountry non disponibile\n",
    "        repl = {\"UK\": \"GBR\", \"EL\": \"GRC\"}\n",
    "        for a2 in a2_vals:\n",
    "            if a2 in repl:\n",
    "                mapping[a2] = repl[a2]\n",
    "    if not mapping:\n",
    "        print(\"[WARN] Impossibile costruire mapping NUTS alpha2 -> ISO3. Installa 'pycountry'.\")\n",
    "    return mapping\n",
    "\n",
    "# Istanzia una volta (usa la tua variabile globale NUTS_GEOJSON_PATH)\n",
    "NUTS_A2_TO_ISO3: Dict[str, str] = build_nuts_a2_to_iso3_map(NUTS_GEOJSON_PATH)\n",
    "\n",
    "def iso3_from_node_label(nodeLabel: str, dataset: str) -> Optional[str]:\n",
    "    \"\"\"Inferisce ISO3 dal nodeLabel a seconda del dataset.\"\"\"\n",
    "    if not isinstance(nodeLabel, str):\n",
    "        return None\n",
    "    ds = (dataset or \"\").upper()\n",
    "    if ds in (\"GADM1\", \"GADM2\"):\n",
    "        m = re.match(r\"^([A-Z]{3})\\.\", nodeLabel)\n",
    "        return m.group(1) if m else None\n",
    "    if ds == \"COUNTY\":\n",
    "        return \"USA\"\n",
    "    if ds == \"NUTS3\":\n",
    "        a2 = nodeLabel[:2].upper()\n",
    "        return NUTS_A2_TO_ISO3.get(a2)\n",
    "    return None\n",
    "\n",
    "def country_name_from_iso3(iso3: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Ritorna un country name leggibile da ISO3 (se 'pycountry' è installato).\"\"\"\n",
    "    if iso3 is None or pd.isna(iso3):\n",
    "        return None\n",
    "    try:\n",
    "        import pycountry\n",
    "        c = pycountry.countries.get(alpha_3=str(iso3))\n",
    "        if c:\n",
    "            return getattr(c, \"common_name\", None) or getattr(c, \"name\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _detect_country_cols(df_map: pd.DataFrame) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Versione permissiva: se non ci sono colonne ISO3/nome nel mapping, restituisce (None, None).\"\"\"\n",
    "    iso_candidates = [\"country_ISO3\", \"ISO3\", \"iso3\", \"COUNTRY_ISO3\", \"country_iso3\"]\n",
    "    name_candidates = [\"country_name\", \"COUNTRY_NAME\", \"country\", \"COUNTRY\", \"NAME_0\"]\n",
    "    iso_col = next((c for c in iso_candidates if c in df_map.columns), None)\n",
    "    name_col = next((c for c in name_candidates if c in df_map.columns), None)\n",
    "    return iso_col, name_col\n",
    "\n",
    "def build_loc2node_mapping(\n",
    "    nodes: pd.DataFrame,\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    allowed_datasets: Iterable[str] = (\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\")\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    location_code (SCI) -> (nodeID, nodeLabel, DatasetDiOrigine, country_ISO3, country_name)\n",
    "    ISO3 viene:\n",
    "      - da df_map se presente; ALTRIMENTI\n",
    "      - inferito da nodeLabel + dataset (NUTS via CNTR_CODE; GADM dal prefisso; COUNTY=USA).\n",
    "    \"\"\"\n",
    "    allowed = {t.upper() for t in allowed_datasets}\n",
    "    nodes_ok = nodes[nodes[\"DatasetDiOrigine\"].str.upper().isin(allowed)].copy()\n",
    "\n",
    "    # target: codici SCI ∩ mapping, limitati ai tipi ammessi\n",
    "    target = select_target_codes(df_sci=df_sci, df_map=df_map, level_types=allowed)\n",
    "\n",
    "    # Costruisci nodeLabel atteso per tipo (come nei nodi)\n",
    "    parts = []\n",
    "    if \"NUTS3\" in allowed:\n",
    "        t = target[target[\"level_type\"].str.upper() == \"NUTS3\"].copy()\n",
    "        t[\"code\"] = t[\"location_code\"]\n",
    "        t[\"nodeLabel\"] = t[\"code\"]\n",
    "        parts.append(t)\n",
    "    if \"GADM2\" in allowed:\n",
    "        t = target[target[\"level_type\"].str.upper() == \"GADM2\"].copy()\n",
    "        t[\"code\"] = t[\"location_code\"].map(normalize_gadm2_mapping_code)\n",
    "        t[\"nodeLabel\"] = t[\"code\"]\n",
    "        parts.append(t)\n",
    "    if \"GADM1\" in allowed:\n",
    "        t = target[target[\"level_type\"].str.upper() == \"GADM1\"].copy()\n",
    "        t[\"code\"] = t[\"location_code\"].map(normalize_gadm1_mapping_code)\n",
    "        t[\"nodeLabel\"] = t[\"code\"]\n",
    "        parts.append(t)\n",
    "    if \"COUNTY\" in allowed:\n",
    "        t = target[target[\"level_type\"].str.upper() == \"COUNTY\"].copy()\n",
    "        t[\"code\"] = t[\"location_code\"].map(normalize_county_mapping_code)\n",
    "        t[\"nodeLabel\"] = \"USA\" + t[\"code\"].astype(str).str.zfill(5)\n",
    "        parts.append(t)\n",
    "\n",
    "    if not parts:\n",
    "        raise ValueError(\"Nessun dataset consentito per costruire il mapping loc->node.\")\n",
    "\n",
    "    mapping_raw = (pd.concat(parts, ignore_index=True)\n",
    "                     .dropna(subset=[\"nodeLabel\"])\n",
    "                     .drop_duplicates([\"location_code\", \"nodeLabel\"]))\n",
    "\n",
    "    # Allinea ai nodi effettivi\n",
    "    loc2node = mapping_raw.merge(\n",
    "        nodes_ok[[\"nodeID\", \"nodeLabel\", \"DatasetDiOrigine\"]],\n",
    "        on=\"nodeLabel\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # --- Paese: usa df_map se disponibile, altrimenti inferisci ---\n",
    "    iso_col, name_col = _detect_country_cols(df_map)\n",
    "    if iso_col is not None:\n",
    "        loc2country = df_map[[\"location_code\", iso_col]].copy().rename(columns={iso_col: \"country_ISO3\"})\n",
    "        if name_col and name_col in df_map.columns and name_col != iso_col:\n",
    "            loc2country[\"country_name\"] = df_map[name_col]\n",
    "        else:\n",
    "            loc2country[\"country_name\"] = pd.NA\n",
    "        loc2node = loc2node.merge(loc2country, on=\"location_code\", how=\"left\")\n",
    "    else:\n",
    "        loc2node[\"country_ISO3\"] = pd.NA\n",
    "        loc2node[\"country_name\"] = pd.NA\n",
    "\n",
    "    # Fallback ISO3 per i mancanti: inferisci da nodeLabel+dataset\n",
    "    miss_iso = loc2node[\"country_ISO3\"].isna()\n",
    "    if miss_iso.any():\n",
    "        loc2node.loc[miss_iso, \"country_ISO3\"] = loc2node.loc[miss_iso].apply(\n",
    "            lambda r: iso3_from_node_label(r[\"nodeLabel\"], r[\"DatasetDiOrigine\"]), axis=1\n",
    "        ).astype(\"string\")\n",
    "\n",
    "    # Fallback country_name se mancante: prova via pycountry\n",
    "    miss_name = loc2node[\"country_name\"].isna() if \"country_name\" in loc2node.columns else pd.Series(True, index=loc2node.index)\n",
    "    if miss_name.any():\n",
    "        loc2node.loc[miss_name, \"country_name\"] = loc2node.loc[miss_name, \"country_ISO3\"].map(country_name_from_iso3)\n",
    "\n",
    "    # Deduplica per location_code\n",
    "    loc2node = (loc2node\n",
    "                .sort_values([\"location_code\", \"DatasetDiOrigine\"])  # priorità stabilita a monte dai nodi\n",
    "                .drop_duplicates(\"location_code\"))\n",
    "\n",
    "    assert loc2node[\"nodeID\"].isna().sum() == 0, \"Mapping loc->node con nodeID mancanti\"\n",
    "    return loc2node[[\"location_code\", \"nodeID\", \"nodeLabel\", \"DatasetDiOrigine\", \"country_ISO3\", \"country_name\"]]\n",
    "\n",
    "def build_edge_lists(\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    nodes: pd.DataFrame,\n",
    "    allowed_datasets: Iterable[str] = (\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\"),\n",
    "    output_dir: str = \".\",\n",
    "    general_filename: str = \"edges_all.csv\",\n",
    "    per_country: bool = True,\n",
    "    country_column_mode: str = \"from\"  # 'from' | 'to' | 'common_or_null'\n",
    ") -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Output:\n",
    "      - edges_all.csv: (nodeID_from,nodeID_to,country_name,country_ISO3,weight)\n",
    "      - edges_by_country/edges_<ISO3>.csv (solo intra-paese)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Mapping location_code (SCI) -> nodo + paese\n",
    "    loc2node = build_loc2node_mapping(nodes, df_sci, df_map, allowed_datasets)\n",
    "\n",
    "    # Join SCI con mapping from/to\n",
    "    sci = df_sci[[\"user_loc\", \"fr_loc\", \"scaled_sci\"]].copy()\n",
    "    sci[\"user_loc\"] = _normalize_location_code(sci[\"user_loc\"])\n",
    "    sci[\"fr_loc\"]   = _normalize_location_code(sci[\"fr_loc\"])\n",
    "\n",
    "    left  = loc2node.add_prefix(\"from_\")\n",
    "    right = loc2node.add_prefix(\"to_\")\n",
    "\n",
    "    e = (sci\n",
    "         .merge(left,  left_on=\"user_loc\", right_on=\"from_location_code\", how=\"inner\")\n",
    "         .merge(right, left_on=\"fr_loc\",  right_on=\"to_location_code\",   how=\"inner\"))\n",
    "\n",
    "    # Colonne paese da esporre\n",
    "    if country_column_mode == \"to\":\n",
    "        country_iso  = e[\"to_country_ISO3\"]\n",
    "        country_name = e[\"to_country_name\"]\n",
    "    elif country_column_mode == \"common_or_null\":\n",
    "        same = e[\"from_country_ISO3\"].notna() & (e[\"from_country_ISO3\"] == e[\"to_country_ISO3\"])\n",
    "        country_iso  = e[\"from_country_ISO3\"].where(same)\n",
    "        country_name = e[\"from_country_name\"].where(same)\n",
    "    else:  # default: 'from'\n",
    "        country_iso  = e[\"from_country_ISO3\"]\n",
    "        country_name = e[\"from_country_name\"]\n",
    "\n",
    "    edges_all = pd.DataFrame({\n",
    "        \"nodeID_from\": e[\"from_nodeID\"].astype(int),\n",
    "        \"nodeID_to\":   e[\"to_nodeID\"].astype(int),\n",
    "        \"country_name\": country_name.astype(\"string\"),\n",
    "        \"country_ISO3\": country_iso.astype(\"string\"),\n",
    "        \"weight\":      e[\"scaled_sci\"].astype(float),\n",
    "    })\n",
    "\n",
    "    # Salva generale\n",
    "    out_all = os.path.join(output_dir, general_filename)\n",
    "    edges_all.to_csv(out_all, index=False)\n",
    "    print(f\"✅ Salvato edges generale: {out_all} (righe: {len(edges_all):,})\")\n",
    "\n",
    "    # Edges intra-paese\n",
    "    edges_by_iso: Dict[str, pd.DataFrame] = {}\n",
    "    if per_country:\n",
    "        intra_mask = e[\"from_country_ISO3\"].notna() & (e[\"from_country_ISO3\"] == e[\"to_country_ISO3\"])\n",
    "        intra = e.loc[intra_mask].copy()\n",
    "        if len(intra):\n",
    "            by_dir = os.path.join(output_dir, \"edges_by_country\")\n",
    "            os.makedirs(by_dir, exist_ok=True)\n",
    "            intra[\"country_ISO3\"] = intra[\"from_country_ISO3\"].astype(str)\n",
    "            intra[\"country_name\"] = intra[\"from_country_name\"]\n",
    "\n",
    "            for iso3, grp in intra.groupby(\"country_ISO3\"):\n",
    "                df_iso = pd.DataFrame({\n",
    "                    \"nodeID_from\": grp[\"from_nodeID\"].astype(int),\n",
    "                    \"nodeID_to\":   grp[\"to_nodeID\"].astype(int),\n",
    "                    \"country_name\": grp[\"country_name\"].astype(\"string\"),\n",
    "                    \"country_ISO3\": grp[\"country_ISO3\"].astype(\"string\"),\n",
    "                    \"weight\":       grp[\"scaled_sci\"].astype(float)\n",
    "                })\n",
    "                edges_by_iso[iso3] = df_iso\n",
    "                file_iso = os.path.join(by_dir, f\"edges_{iso3}.csv\")\n",
    "                df_iso.to_csv(file_iso, index=False)\n",
    "            print(f\"✅ Salvati {len(edges_by_iso)} file intra-paese in: {by_dir}\")\n",
    "        else:\n",
    "            print(\"[INFO] Nessun edge intra-paese trovato con i filtri correnti.\")\n",
    "\n",
    "    return edges_all, edges_by_iso\n",
    "\n",
    "# ==========================\n",
    "# ESEMPIO DI ESECUZIONE\n",
    "# ==========================\n",
    "ALLOWED_DATASETS = (\"NUTS3\", \"GADM2\", \"GADM1\", \"COUNTY\")\n",
    "\n",
    "df_edges_all, edges_per_country = build_edge_lists(\n",
    "    df_sci=df_sci,\n",
    "    df_map=df_map,\n",
    "    nodes=node_list,\n",
    "    allowed_datasets=ALLOWED_DATASETS,\n",
    "    output_dir=\".\",\n",
    "    general_filename=\"edges_all.csv\",\n",
    "    per_country=True,\n",
    "    country_column_mode=\"from\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06cd28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Salvato df_sci → cache/df_sci.pkl\n",
      "✅ Salvato df_map → cache/df_map.pkl\n",
      "✅ Salvato nuts_pts → cache/nuts_pts.pkl\n",
      "✅ Salvato gadm2_pts → cache/gadm2_pts.pkl\n",
      "✅ Salvato gadm1_pts → cache/gadm1_pts.pkl\n",
      "✅ Salvato county_pts → cache/county_pts.pkl\n",
      "✅ Salvato target → cache/target.pkl\n",
      "✅ Salvato nuts_nodes → cache/nuts_nodes.pkl\n",
      "✅ Salvato gadm2_nodes → cache/gadm2_nodes.pkl\n",
      "✅ Salvato gadm1_nodes → cache/gadm1_nodes.pkl\n",
      "✅ Salvato county_nodes → cache/county_nodes.pkl\n",
      "✅ Salvato node_list → cache/node_list.pkl\n",
      "✅ Salvato df_edges_all → cache/df_edges_all.pkl\n",
      "✅ Salvato edges_per_country → cache/edges_per_country.pkl\n",
      "✅ Salvato coverage_by_type → cache/coverage_by_type.pkl\n",
      "✅ Salvato coverage_by_country → cache/coverage_by_country.pkl\n",
      "✅ Salvato node_id_map → cache/node_id_map.pkl\n",
      "[INFO] Target codes selezionati: 7,936 (tipi: ['COUNTY', 'GADM1', 'GADM2', 'NUTS3'])\n",
      "✅ Salvato loc2node → cache/loc2node.pkl\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "# variabili principali già create dal tuo script\n",
    "names = [\n",
    "    \"df_sci\",\"df_map\",\n",
    "    \"nuts_pts\",\"gadm2_pts\",\"gadm1_pts\",\"county_pts\",\n",
    "    \"target\",\n",
    "    \"nuts_nodes\",\"gadm2_nodes\",\"gadm1_nodes\",\"county_nodes\",\n",
    "    \"node_list\",\n",
    "    \"df_edges_all\",\"edges_per_country\",\n",
    "    \"coverage_by_type\",\"coverage_by_country\"\n",
    "]\n",
    "\n",
    "# salva se esistono nel namespace\n",
    "for n in names:\n",
    "    if n in globals():\n",
    "        pd.to_pickle(globals()[n], f\"cache/{n}.pkl\")\n",
    "        print(f\"✅ Salvato {n} → cache/{n}.pkl\")\n",
    "    else:\n",
    "        print(f\"[WARN] Variabile non trovata: {n}\")\n",
    "\n",
    "# comodi extra ------------------------------------------\n",
    "\n",
    "# mappa rapida nodeLabel -> nodeID\n",
    "if \"node_list\" in globals():\n",
    "    node_id_map = node_list.set_index(\"nodeLabel\")[\"nodeID\"].to_dict()\n",
    "    pd.to_pickle(node_id_map, \"cache/node_id_map.pkl\")\n",
    "    print(\"✅ Salvato node_id_map → cache/node_id_map.pkl\")\n",
    "\n",
    "# mapping location_code (SCI) -> nodo (se vuoi riusarli senza rifare join)\n",
    "try:\n",
    "    ALLOWED_DATASETS = (\"NUTS3\",\"GADM2\",\"GADM1\",\"COUNTY\")\n",
    "    loc2node = build_loc2node_mapping(node_list, df_sci, df_map, ALLOWED_DATASETS)\n",
    "    pd.to_pickle(loc2node, \"cache/loc2node.pkl\")\n",
    "    print(\"✅ Salvato loc2node → cache/loc2node.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] loc2node non salvato: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca723ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
