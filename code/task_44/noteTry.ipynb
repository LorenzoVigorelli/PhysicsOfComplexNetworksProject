{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e0c2e5",
   "metadata": {},
   "source": [
    "# SCI → Mapping → Nodes: lettura, copertura e costruzione nodi\n",
    "\n",
    "Questo notebook:\n",
    "1) legge il TSV SCI e il mapping livelli;\n",
    "2) fa controlli e un mini-report di copertura;\n",
    "3) costruisce la lista nodi (NUTS3 / GADM2 / US COUNTIES) con lat/lon;\n",
    "4) salva `node_list.csv`.\n",
    "\n",
    "> Imposta prima i percorsi in **Cella 1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072108ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, Dict, Optional, Tuple, List\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from IPython.display import display\n",
    "\n",
    "# === Percorsi (modifica se necessario) ===\n",
    "TSV_PATH = \"data/gadm1_nuts3_counties-gadm1_nuts3_counties - FB Social Connectedness Index - October 2021.tsv\"\n",
    "MAP_PATH = \"data/gadm1_nuts3_counties_levels.csv\"\n",
    "\n",
    "# Geodati\n",
    "NUTS_GEOJSON_PATH   = \"data/NUTS_RG_60M_2016_4326_LEVL_3.geojson\"\n",
    "US_COUNTIES_PATH    = \"data/us-county-boundaries.geojson\"\n",
    "GADM_GPKG_PATH      = \"data/gadm_410.gpkg\"   # layer tipico: \"gadm_410\"\n",
    "\n",
    "# Parametri lettura SCI\n",
    "SCI_COLS  = [\"user_loc\", \"fr_loc\", \"scaled_sci\"]\n",
    "DTYPE_MAP = {\"user_loc\": \"string\", \"fr_loc\": \"string\", \"scaled_sci\": \"float64\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bcd065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_location_code(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalizza i codici location: stringa, strip, upper (non tocca NaN).\n",
    "    \"\"\"\n",
    "    return s.astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, required: Iterable[str], where: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Verifica che il DataFrame contenga le colonne richieste.\n",
    "    \"\"\"\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        loc = f\" in {where}\" if where else \"\"\n",
    "        raise ValueError(f\"Mancano colonne{loc}: {missing}\")\n",
    "\n",
    "def _preview(df: pd.DataFrame, name: str = \"DataFrame\", n: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Stampa anteprima, schema e numero righe.\n",
    "    \"\"\"\n",
    "    print(f\"== {name} – prime righe ==\")\n",
    "    display(df.head(n))\n",
    "    print(\"\\nSchema:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nNumero righe: {len(df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c0b06dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== SCI (TSV) – prime righe ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_loc</th>\n",
       "      <th>fr_loc</th>\n",
       "      <th>scaled_sci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABW</td>\n",
       "      <td>ABW</td>\n",
       "      <td>11264841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO1</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO10</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO11</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABW</td>\n",
       "      <td>AGO12</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_loc fr_loc  scaled_sci\n",
       "0      ABW    ABW  11264841.0\n",
       "1      ABW   AGO1        38.0\n",
       "2      ABW  AGO10        34.0\n",
       "3      ABW  AGO11        32.0\n",
       "4      ABW  AGO12        23.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "user_loc      string[python]\n",
      "fr_loc        string[python]\n",
      "scaled_sci           float64\n",
      "dtype: object\n",
      "\n",
      "Numero righe: 63,824,121\n",
      "== Mapping livelli – prime righe ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_code</th>\n",
       "      <th>level_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AND</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATG</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABW</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BHS</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRB</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  location_code level_type\n",
       "0           AND    country\n",
       "1           ATG    country\n",
       "2           ABW    country\n",
       "3           BHS    country\n",
       "4           BRB    country"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema:\n",
      "location_code    string[python]\n",
      "level_type       string[python]\n",
      "dtype: object\n",
      "\n",
      "Numero righe: 8,008\n",
      "\n",
      "== level_type value_counts ==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "level_type\n",
       "county     3229\n",
       "gadm1      1839\n",
       "nuts3      1522\n",
       "gadm2      1370\n",
       "country      48\n",
       "Name: count, dtype: Int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_sci_tsv(\n",
    "    path: str,\n",
    "    sci_cols: Iterable[str] = (\"user_loc\", \"fr_loc\", \"scaled_sci\"),\n",
    "    dtype_map: Optional[Dict[str, str]] = None,\n",
    "    low_memory: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if dtype_map is None:\n",
    "        dtype_map = {\"user_loc\": \"string\", \"fr_loc\": \"string\", \"scaled_sci\": \"float64\"}\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        usecols=list(sci_cols),\n",
    "        dtype=dtype_map,\n",
    "        low_memory=low_memory\n",
    "    )\n",
    "\n",
    "    # normalizza codici\n",
    "    if \"user_loc\" in df.columns:\n",
    "        df[\"user_loc\"] = _normalize_location_code(df[\"user_loc\"])\n",
    "    if \"fr_loc\" in df.columns:\n",
    "        df[\"fr_loc\"] = _normalize_location_code(df[\"fr_loc\"])\n",
    "\n",
    "    _ensure_columns(df, sci_cols, where=\"SCI TSV\")\n",
    "    return df\n",
    "\n",
    "def load_levels_mapping(\n",
    "    path: str,\n",
    "    usecols: Iterable[str] = (\"key\", \"level\"),\n",
    "    rename_map: Dict[str, str] = {\"key\": \"location_code\", \"level\": \"level_type\"},\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, usecols=list(usecols), dtype=\"string\").rename(columns=rename_map)\n",
    "    _ensure_columns(df, [\"location_code\", \"level_type\"], where=\"Mapping livelli\")\n",
    "    df[\"location_code\"] = _normalize_location_code(df[\"location_code\"])\n",
    "    return df\n",
    "\n",
    "def quick_read_inputs(\n",
    "    tsv_path: str,\n",
    "    map_path: str,\n",
    "    sci_cols: Iterable[str] = (\"user_loc\", \"fr_loc\", \"scaled_sci\"),\n",
    "    dtype_map: Optional[Dict[str, str]] = None\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_sci = load_sci_tsv(tsv_path, sci_cols=sci_cols, dtype_map=dtype_map)\n",
    "    _preview(df_sci, name=\"SCI (TSV)\")\n",
    "    df_map = load_levels_mapping(map_path)\n",
    "    _preview(df_map, name=\"Mapping livelli\")\n",
    "    return df_sci, df_map\n",
    "\n",
    "# Esecuzione lettura\n",
    "df_sci, df_map = quick_read_inputs(TSV_PATH, MAP_PATH, sci_cols=SCI_COLS, dtype_map=DTYPE_MAP)\n",
    "\n",
    "# Facoltativo: distribuzione tipi livello\n",
    "print(\"\\n== level_type value_counts ==\")\n",
    "display(df_map['level_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae411485",
   "metadata": {},
   "source": [
    "ABOUT COVERAGE IN THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd519aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_location_codes(df_sci: pd.DataFrame) -> pd.Series:\n",
    "    _ensure_columns(df_sci, [\"user_loc\", \"fr_loc\"], where=\"SCI\")\n",
    "    u = _normalize_location_code(df_sci[\"user_loc\"])\n",
    "    v = _normalize_location_code(df_sci[\"fr_loc\"])\n",
    "    return pd.Index(u).append(pd.Index(v)).astype(\"string\").unique()\n",
    "\n",
    "def get_mapped_codes(df_map: pd.DataFrame) -> pd.Series:\n",
    "    _ensure_columns(df_map, [\"location_code\"], where=\"Mapping livelli\")\n",
    "    return _normalize_location_code(df_map[\"location_code\"]).dropna().unique()\n",
    "\n",
    "def compute_node_coverage(df_sci: pd.DataFrame, df_map: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
    "    sci_codes = pd.Series(get_unique_location_codes(df_sci), name=\"location_code\")\n",
    "    map_codes = pd.Series(get_mapped_codes(df_map), name=\"location_code\")\n",
    "\n",
    "    total_codes = sci_codes.size\n",
    "    mapped_mask = sci_codes.isin(set(map_codes))\n",
    "    mapped_count = int(mapped_mask.sum())\n",
    "    unmapped = (\n",
    "        sci_codes[~mapped_mask].to_frame().drop_duplicates()\n",
    "        .sort_values(\"location_code\").reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    summary = {\n",
    "        \"total_unique_codes\": int(total_codes),\n",
    "        \"mapped_unique_codes\": int(mapped_count),\n",
    "        \"unmapped_unique_codes\": int(total_codes - mapped_count),\n",
    "        \"node_coverage_pct\": (mapped_count / total_codes * 100.0) if total_codes else 0.0,\n",
    "    }\n",
    "    return unmapped, summary\n",
    "\n",
    "def compute_edge_coverage(df_sci: pd.DataFrame, df_map: pd.DataFrame) -> Dict[str, float]:\n",
    "    _ensure_columns(df_sci, [\"user_loc\", \"fr_loc\"], where=\"SCI\")\n",
    "    mapped_set = set(get_mapped_codes(df_map))\n",
    "\n",
    "    u = _normalize_location_code(df_sci[\"user_loc\"])\n",
    "    v = _normalize_location_code(df_sci[\"fr_loc\"])\n",
    "\n",
    "    both_mapped_mask = u.isin(mapped_set) & v.isin(mapped_set)\n",
    "    total_rows = int(len(df_sci))\n",
    "    valid_rows = int(both_mapped_mask.sum())\n",
    "\n",
    "    return {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"valid_rows_both_mapped\": valid_rows,\n",
    "        \"edge_coverage_pct\": (valid_rows / total_rows * 100.0) if total_rows else 0.0,\n",
    "    }\n",
    "\n",
    "def compute_country_coverage(\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    iso_col: str = \"country_ISO3\",\n",
    "    keep_only_intra_country: bool = True\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    if iso_col not in df_map.columns:\n",
    "        print(f\"[AVVISO] Colonna '{iso_col}' assente nel mapping: copertura per paese non calcolata.\")\n",
    "        return None\n",
    "\n",
    "    _ensure_columns(df_sci, [\"user_loc\", \"fr_loc\"], where=\"SCI\")\n",
    "\n",
    "    df_map_local = df_map[[\"location_code\", iso_col]].copy()\n",
    "    df_map_local[\"location_code\"] = _normalize_location_code(df_map_local[\"location_code\"])\n",
    "    code2iso = (\n",
    "        df_map_local.dropna().drop_duplicates(\"location_code\")\n",
    "        .set_index(\"location_code\")[iso_col]\n",
    "    )\n",
    "\n",
    "    sci = df_sci[[\"user_loc\", \"fr_loc\"]].copy()\n",
    "    sci[\"user_loc\"] = _normalize_location_code(sci[\"user_loc\"])\n",
    "    sci[\"fr_loc\"]   = _normalize_location_code(sci[\"fr_loc\"])\n",
    "\n",
    "    sci[\"iso_from\"] = sci[\"user_loc\"].map(code2iso)\n",
    "    sci[\"iso_to\"]   = sci[\"fr_loc\"].map(code2iso)\n",
    "\n",
    "    sci_valid = sci.dropna(subset=[\"iso_from\", \"iso_to\"]).copy()\n",
    "    if keep_only_intra_country:\n",
    "        sci_valid = sci_valid[sci_valid[\"iso_from\"] == sci_valid[\"iso_to\"]].copy()\n",
    "\n",
    "    edges_by_country = (\n",
    "        sci_valid.groupby(\"iso_from\", as_index=False)\n",
    "        .agg(edges=(\"user_loc\", \"size\")).rename(columns={\"iso_from\": iso_col})\n",
    "    )\n",
    "\n",
    "    nodes_from = sci_valid[[\"iso_from\", \"user_loc\"]].rename(columns={\"iso_from\": iso_col, \"user_loc\": \"loc\"})\n",
    "    nodes_to   = sci_valid[[\"iso_to\", \"fr_loc\"]].rename(columns={\"iso_to\": iso_col, \"fr_loc\": \"loc\"})\n",
    "    nodes_all  = pd.concat([nodes_from, nodes_to], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    nodes_by_country = nodes_all.groupby(iso_col, as_index=False).agg(nodes=(\"loc\", \"nunique\"))\n",
    "\n",
    "    country_cov = edges_by_country.merge(nodes_by_country, on=iso_col, how=\"outer\").fillna(0)\n",
    "    country_cov[\"edges\"] = country_cov[\"edges\"].astype(int)\n",
    "    country_cov[\"nodes\"] = country_cov[\"nodes\"].astype(int)\n",
    "\n",
    "    total_edges = int(country_cov[\"edges\"].sum()) if len(country_cov) else 0\n",
    "    country_cov[\"edges_pct\"] = (100.0 * country_cov[\"edges\"] / total_edges) if total_edges else 0.0\n",
    "\n",
    "    return country_cov.sort_values([\"edges\", \"nodes\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "def coverage_report(df_sci: pd.DataFrame, df_map: pd.DataFrame, top_n: int = 10) -> None:\n",
    "    unmapped_nodes, node_summary = compute_node_coverage(df_sci, df_map)\n",
    "    print(\"== Copertura NODI ==\")\n",
    "    for k, v in node_summary.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "    if len(unmapped_nodes) > 0:\n",
    "        print(f\"\\nEsempi codici NON mappati ({min(10, len(unmapped_nodes))}):\")\n",
    "        display(unmapped_nodes.head(10))\n",
    "\n",
    "    edge_summary = compute_edge_coverage(df_sci, df_map)\n",
    "    print(\"\\n== Copertura ARCHI ==\")\n",
    "    for k, v in edge_summary.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "    country_cov = compute_country_coverage(df_sci, df_map)\n",
    "    if country_cov is not None and len(country_cov):\n",
    "        print(\"\\n== Copertura PER PAESE (prime righe) ==\")\n",
    "        display(country_cov.head(top_n))\n",
    "    else:\n",
    "        print(\"\\n== Copertura PER PAESE ==\")\n",
    "        print(\"Mapping senza 'country_ISO3' → se vuoi questa sezione, aggiungi la colonna a df_map.\")\n",
    "\n",
    "# Esecuzione report\n",
    "coverage_report(df_sci, df_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d62fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _representative_points(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].representative_point()\n",
    "    gdf[\"latitude\"] = gdf.geometry.y\n",
    "    gdf[\"longitude\"] = gdf.geometry.x\n",
    "    return gdf\n",
    "\n",
    "def load_nuts3_points(nuts_geojson_path: str, code_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(nuts_geojson_path)\n",
    "    levl_col = next((c for c in [\"LEVL_CODE\", \"LEVL\", \"LEVEL\"] if c in gdf.columns), None)\n",
    "    if levl_col is not None:\n",
    "        gdf = gdf[gdf[levl_col].astype(str).isin([\"3\", 3])].copy()\n",
    "\n",
    "    candidates = [code_col] if code_col else None\n",
    "    if not candidates or candidates == [None]:\n",
    "        candidates = [c for c in [\"NUTS_ID\", \"nuts_id\", \"ID\", \"id\"] if c in gdf.columns]\n",
    "    if not candidates:\n",
    "        raise ValueError(\"Non trovo una colonna codice per NUTS (es. 'NUTS_ID'). Passa code_col=...\")\n",
    "\n",
    "    cc = candidates[0]\n",
    "    gdf = gdf.rename(columns={cc: \"code\"})\n",
    "    gdf = _representative_points(gdf[[\"code\", \"geometry\"]].dropna())\n",
    "    gdf[\"code\"] = _normalize_location_code(gdf[\"code\"])\n",
    "    return gdf[[\"code\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "\n",
    "def load_gadm2_points(gadm_gpkg_path: str, layer: str = \"gadm_410\", code_col: str = \"GID_2\") -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(gadm_gpkg_path, layer=layer)\n",
    "    if code_col not in gdf.columns:\n",
    "        raise ValueError(f\"La colonna {code_col!r} non esiste nel layer {layer}. Colonne trovate: {list(gdf.columns)}\")\n",
    "\n",
    "    gdf2 = gdf[~gdf[code_col].isna()].copy().rename(columns={code_col: \"code\"})\n",
    "    gdf2 = gdf2.to_crs(4326)\n",
    "    gdf2[\"geometry\"] = gdf2.geometry.representative_point()\n",
    "    gdf2[\"latitude\"] = gdf2.geometry.y\n",
    "    gdf2[\"longitude\"] = gdf2.geometry.x\n",
    "    gdf2[\"code\"] = gdf2[\"code\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "    out = gdf2[[\"code\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "    print(f\"[GADM2] Caricate {len(out):,} unità ADM2 uniche da {gadm_gpkg_path}\")\n",
    "    return out\n",
    "\n",
    "def load_us_counties_points(counties_geojson_path: str, code_col: Optional[str] = None) -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(counties_geojson_path)\n",
    "    candidates = [code_col] if code_col else None\n",
    "    if not candidates or candidates == [None]:\n",
    "        candidates = [c for c in [\"GEOID\", \"geoid\", \"FIPS\", \"fips\"] if c in gdf.columns]\n",
    "    if not candidates:\n",
    "        raise ValueError(\"Non trovo una colonna codice per US counties (es. 'GEOID'). Passa code_col=...\")\n",
    "\n",
    "    cc = candidates[0]\n",
    "    gdf = gdf.rename(columns={cc: \"code\"})\n",
    "    gdf[\"code\"] = gdf[\"code\"].astype(str).str.zfill(5)\n",
    "    gdf = _representative_points(gdf[[\"code\", \"geometry\"]].dropna())\n",
    "    gdf[\"code\"] = _normalize_location_code(gdf[\"code\"])\n",
    "    return gdf[[\"code\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "\n",
    "def load_gadm1_points(gadm_gpkg_path: str, layer: str = \"gadm_410\", code_col: str = \"GID_1\") -> pd.DataFrame:\n",
    "    gdf = gpd.read_file(gadm_gpkg_path, layer=layer)\n",
    "    if code_col not in gdf.columns:\n",
    "        raise ValueError(f\"Colonna {code_col!r} assente in {layer}. Colonne: {list(gdf.columns)}\")\n",
    "    gdf1 = gdf[~gdf[code_col].isna()].copy().rename(columns={code_col: \"code\"})\n",
    "    gdf1 = gdf1.to_crs(4326)\n",
    "    gdf1[\"geometry\"] = gdf1.geometry.representative_point()\n",
    "    gdf1[\"latitude\"] = gdf1.geometry.y\n",
    "    gdf1[\"longitude\"] = gdf1.geometry.x\n",
    "    gdf1[\"code\"] = gdf1[\"code\"].astype(\"string\").str.strip().str.upper()\n",
    "    return gdf1[[\"code\", \"latitude\", \"longitude\"]].drop_duplicates(\"code\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30792ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_target_codes(\n",
    "    df_sci: pd.DataFrame,\n",
    "    df_map: pd.DataFrame,\n",
    "    level_types: Iterable[str] = (\"NUTS3\", \"GADM1\", \"COUNTY\"),\n",
    "    sci_required: Iterable[str] = (\"user_loc\", \"fr_loc\"),\n",
    "    map_required: Iterable[str] = (\"location_code\", \"level_type\"),\n",
    ") -> pd.DataFrame:\n",
    "    # Controlli colonne corretti\n",
    "    _ensure_columns(df_sci, sci_required, \"SCI\")\n",
    "    _ensure_columns(df_map, map_required, \"Mapping\")\n",
    "\n",
    "    # Codici presenti nello SCI (user_loc ∪ fr_loc)\n",
    "    sci_codes = pd.Index(_normalize_location_code(df_sci[\"user_loc\"])) \\\n",
    "                  .append(pd.Index(_normalize_location_code(df_sci[\"fr_loc\"]))) \\\n",
    "                  .unique()\n",
    "\n",
    "    # Filtra mapping per i tipi desiderati e normalizza\n",
    "    df_map2 = df_map.copy()\n",
    "    df_map2[\"location_code\"] = _normalize_location_code(df_map2[\"location_code\"])\n",
    "    df_map2[\"level_type\"] = df_map2[\"level_type\"].astype(\"string\")\n",
    "\n",
    "    wanted = {t.upper() for t in level_types}\n",
    "    df_map2 = df_map2[df_map2[\"level_type\"].str.upper().isin(wanted)]\n",
    "\n",
    "    # Intersezione SCI ∩ mapping e deduplica\n",
    "    target = (df_map2[df_map2[\"location_code\"].isin(set(sci_codes))]\n",
    "              .drop_duplicates(subset=[\"location_code\", \"level_type\"])\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    print(f\"[INFO] Target codes selezionati: {len(target):,} \"\n",
    "          f\"(tipi: {sorted(target['level_type'].str.upper().unique())})\")\n",
    "    return target\n",
    "\n",
    "\n",
    "\n",
    "def normalize_gadm1_mapping_code(code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Esempi attesi nel mapping: 'BGD1', 'IND23', ...\n",
    "    Output (GADM v4): 'ISO3.ADM1_1'  es. 'BGD.1_1'\n",
    "    \"\"\"\n",
    "    if code is None or pd.isna(code): \n",
    "        return None\n",
    "    s = str(code).strip().upper()\n",
    "    m = re.fullmatch(r'([A-Z]{3})(\\d+)', s)\n",
    "    if not m:\n",
    "        return None\n",
    "    iso3, adm1 = m.groups()\n",
    "    return f\"{iso3}.{int(adm1)}_1\"\n",
    "\n",
    "def normalize_county_mapping_code(code: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    'USA06091' -> '06091' (GEOID a 5 cifre).\n",
    "    \"\"\"\n",
    "    if code is None or pd.isna(code):\n",
    "        return None\n",
    "    s = str(code).strip().upper()\n",
    "    if s.startswith(\"USA\"):\n",
    "        s = s[3:]\n",
    "    s = re.sub(r'\\D', '', s)\n",
    "    if len(s) == 5:\n",
    "        return s\n",
    "    if 1 <= len(s) <= 5:\n",
    "        return s.zfill(5)\n",
    "    return None\n",
    "\n",
    "def build_nodes_for_type_with_transform(\n",
    "    target_codes: pd.DataFrame,\n",
    "    type_name: str,\n",
    "    source_df_points: pd.DataFrame,\n",
    "    transform_fn=None\n",
    ") -> pd.DataFrame:\n",
    "    mask = target_codes[\"level_type\"].str.upper() == type_name.upper()\n",
    "    wanted = target_codes.loc[mask, [\"location_code\"]].copy()\n",
    "    wanted[\"location_code\"] = wanted[\"location_code\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "    if transform_fn is not None:\n",
    "        wanted[\"code\"] = wanted[\"location_code\"].map(transform_fn)\n",
    "    else:\n",
    "        wanted[\"code\"] = wanted[\"location_code\"]\n",
    "\n",
    "    wanted = wanted.dropna(subset=[\"code\"]).drop_duplicates(\"code\")\n",
    "\n",
    "    pts = source_df_points.copy()\n",
    "    pts[\"code\"] = pts[\"code\"].astype(\"string\").str.strip().str.upper()\n",
    "\n",
    "    out = wanted.merge(pts, on=\"code\", how=\"inner\")\n",
    "    out = out.rename(columns={\"code\": \"nodeLabel\"})\n",
    "    out[\"level_type\"] = type_name.upper()\n",
    "    return out[[\"nodeLabel\", \"latitude\", \"longitude\", \"level_type\"]].drop_duplicates(\"nodeLabel\")\n",
    "\n",
    "def build_nodes_for_type(\n",
    "    target_codes: pd.DataFrame,\n",
    "    type_name: str,\n",
    "    source_df_points: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    return build_nodes_for_type_with_transform(\n",
    "        target_codes=target_codes,\n",
    "        type_name=type_name,\n",
    "        source_df_points=source_df_points,\n",
    "        transform_fn=None\n",
    "    )\n",
    "\n",
    "def assemble_node_list(\n",
    "    nuts_nodes: Optional[pd.DataFrame] = None,\n",
    "    gadm_nodes: Optional[pd.DataFrame] = None,   # qui passeremo GADM1\n",
    "    county_nodes: Optional[pd.DataFrame] = None\n",
    ") -> pd.DataFrame:\n",
    "    frames = [df for df in [nuts_nodes, gadm_nodes, county_nodes] if df is not None and len(df)]\n",
    "    if not frames:\n",
    "        raise ValueError(\"Nessun nodo fornito.\")\n",
    "\n",
    "    nodes = pd.concat(frames, ignore_index=True).drop_duplicates(\"nodeLabel\")\n",
    "    nodes = nodes.sort_values(\"nodeLabel\").reset_index(drop=True)\n",
    "    nodes.insert(0, \"nodeID\", range(1, len(nodes) + 1))\n",
    "    return nodes[[\"nodeID\", \"nodeLabel\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "\n",
    "def build_nodes_for_type(\n",
    "    target_codes: pd.DataFrame,\n",
    "    type_name: str,\n",
    "    source_df_points: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    return build_nodes_for_type_with_transform(\n",
    "        target_codes=target_codes,\n",
    "        type_name=type_name,\n",
    "        source_df_points=source_df_points,\n",
    "        transform_fn=None\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49b8f2",
   "metadata": {},
   "source": [
    "OPTIONAL JUST CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a6d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elenco layer del GPKG GADM (utile per capire i nomi giusti)\n",
    "layers = fiona.listlayers(GADM_GPKG_PATH)\n",
    "print(f\"Layer trovati in {GADM_GPKG_PATH}:\")\n",
    "for i, lyr in enumerate(layers, 1):\n",
    "    print(f\"{i:>2}. {lyr}\")\n",
    "\n",
    "# ATTENZIONE: se la tua versione di GeoPandas NON supporta 'rows=' in read_file,\n",
    "# rimuovi 'rows=5' qui sotto (è solo una lettura di anteprima).\n",
    "try:\n",
    "    nuts_sample = gpd.read_file(NUTS_GEOJSON_PATH, rows=5)\n",
    "except TypeError:\n",
    "    nuts_sample = gpd.read_file(NUTS_GEOJSON_PATH)\n",
    "print(\"\\nColonne disponibili in NUTS3 GeoJSON:\")\n",
    "print(list(nuts_sample.columns))\n",
    "display(nuts_sample.head())\n",
    "\n",
    "try:\n",
    "    counties_sample = gpd.read_file(US_COUNTIES_PATH, rows=5)\n",
    "except TypeError:\n",
    "    counties_sample = gpd.read_file(US_COUNTIES_PATH)\n",
    "print(\"\\nColonne disponibili in US counties GeoJSON:\")\n",
    "print(list(counties_sample.columns))\n",
    "display(counties_sample.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41770483",
   "metadata": {},
   "source": [
    "OPTIONAL JUST CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c363e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_gadm2(gpkg_path: str, layer: str = \"gadm_410\", prefer_code_cols=(\"GID_2\",\"ID_2\",\"GID2\")):\n",
    "    gdf = gpd.read_file(gpkg_path, layer=layer)\n",
    "    print(f\"[INFO] Caricato layer='{layer}' con {len(gdf):,} feature\")\n",
    "    print(f\"[INFO] CRS: {gdf.crs}\")\n",
    "    print(\"[INFO] Colonne disponibili:\")\n",
    "    print(list(gdf.columns))\n",
    "\n",
    "    cols_up = {c.upper(): c for c in gdf.columns}\n",
    "    code_col = None\n",
    "    for pref in prefer_code_cols:\n",
    "        if pref in cols_up:\n",
    "            code_col = cols_up[pref]\n",
    "            break\n",
    "    if code_col is None:\n",
    "        candidates = [orig for up, orig in cols_up.items() if up.endswith(\"_2\") or \"GID\" in up]\n",
    "        if not candidates:\n",
    "            raise ValueError(\"Non trovo una colonna codice per ADM2 (tipo 'GID_2' o 'ID_2').\")\n",
    "        code_col = candidates[0]\n",
    "\n",
    "    print(f\"[INFO] Colonna codice ADM2 individuata: {code_col!r}\")\n",
    "    gdf_adm2 = gdf[~gdf[code_col].isna()].copy()\n",
    "    print(f\"[INFO] Righe ADM2 (non-NaN su {code_col}): {len(gdf_adm2):,}\")\n",
    "\n",
    "    unique_codes = gdf_adm2[code_col].astype(\"string\").str.strip().str.upper().nunique()\n",
    "    print(f\"[INFO] Codici ADM2 unici: {unique_codes:,}\")\n",
    "\n",
    "    geom_types = gdf_adm2.geom_type.value_counts().to_dict()\n",
    "    print(f\"[INFO] Geometrie (conteggio per tipo): {geom_types}\")\n",
    "\n",
    "    candidates_name = [c for c in [\"NAME_0\",\"NAME_1\",\"NAME_2\",\"GID_0\",\"GID_1\",\"GID_2\"] if c in gdf_adm2.columns]\n",
    "    show_cols = [code_col] + candidates_name\n",
    "    show_cols = [c for c in dict.fromkeys(show_cols)]  # dedupe preservando ordine\n",
    "\n",
    "    print(\"\\n== Anteprima ADM2 (prime 5 righe) ==\")\n",
    "    display(gdf_adm2[show_cols].head())\n",
    "\n",
    "    if any(c in gdf_adm2.columns for c in [\"NAME_0\",\"NAME_1\",\"NAME_2\",\"GID_0\",\"GID_1\",\"GID_2\"]):\n",
    "        print(\"\\n== Missing count su colonne comuni ==\")\n",
    "        check_cols = [c for c in [\"NAME_0\",\"NAME_1\",\"NAME_2\",\"GID_0\",\"GID_1\",\"GID_2\"] if c in gdf_adm2.columns]\n",
    "        display(gdf_adm2[check_cols].isna().sum().to_frame(\"missing\"))\n",
    "\n",
    "    return gdf, gdf_adm2, code_col\n",
    "\n",
    "# Esecuzione diagnostica\n",
    "gdf_full, gdf_adm2, gadm_code_col = inspect_gadm2(GADM_GPKG_PATH, layer=\"gadm_410\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48dfb45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target codes selezionati: 6,566 (tipi: ['COUNTY', 'GADM1', 'NUTS3'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping field geo_point_2d: unsupported OGR type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUTS pts: 1522 — US counties pts: 3233 — GADM1 pts: 3662\n",
      "NUTS3 selezionati: 1522\n",
      "GADM1 selezionati: 1738\n",
      "COUNTY selezionati: 3225\n"
     ]
    }
   ],
   "source": [
    "# 0) Restrizione ai codici presenti in SCI + mapping (NUTS3 / GADM1 / COUNTY)\n",
    "target = select_target_codes(\n",
    "    df_sci=df_sci,\n",
    "    df_map=df_map,\n",
    "    level_types=(\"NUTS3\", \"GADM1\", \"COUNTY\")  # <-- GADM1\n",
    ")\n",
    "\n",
    "# 1) Carico punti per ciascun tipo\n",
    "nuts_pts    = load_nuts3_points(NUTS_GEOJSON_PATH)\n",
    "county_pts  = load_us_counties_points(US_COUNTIES_PATH)\n",
    "gadm1_pts   = load_gadm1_points(GADM_GPKG_PATH, layer=\"gadm_410\", code_col=\"GID_1\")\n",
    "\n",
    "print(\"NUTS pts:\", len(nuts_pts), \"— US counties pts:\", len(county_pts), \"— GADM1 pts:\", len(gadm1_pts))\n",
    "\n",
    "# 2) Join con eventuale trasformazione delle chiavi\n",
    "gadm1_nodes  = build_nodes_for_type_with_transform(target, \"GADM1\",  gadm1_pts,  transform_fn=normalize_gadm1_mapping_code)\n",
    "county_nodes = build_nodes_for_type_with_transform(target, \"COUNTY\", county_pts, transform_fn=normalize_county_mapping_code)\n",
    "nuts_nodes   = build_nodes_for_type_with_transform(target, \"NUTS3\",  nuts_pts,   transform_fn=None)\n",
    "\n",
    "print(\"NUTS3 selezionati:\", len(nuts_nodes))\n",
    "print(\"GADM1 selezionati:\", len(gadm1_nodes))\n",
    "print(\"COUNTY selezionati:\", len(county_nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3213ab7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodeID</th>\n",
       "      <th>nodeLabel</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01001</td>\n",
       "      <td>32.507734</td>\n",
       "      <td>-86.651230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>01003</td>\n",
       "      <td>30.732698</td>\n",
       "      <td>-87.762633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>01005</td>\n",
       "      <td>31.882808</td>\n",
       "      <td>-85.392945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>01007</td>\n",
       "      <td>33.039116</td>\n",
       "      <td>-87.096651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>01009</td>\n",
       "      <td>34.012895</td>\n",
       "      <td>-86.533567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nodeID nodeLabel   latitude  longitude\n",
       "0       1     01001  32.507734 -86.651230\n",
       "1       2     01003  30.732698 -87.762633\n",
       "2       3     01005  31.882808 -85.392945\n",
       "3       4     01007  33.039116 -87.096651\n",
       "4       5     01009  34.012895 -86.533567"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totale nodi: 6,485\n",
      "✅ Salvato: node_list.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Assemblaggio finale (qui 'gadm_nodes' = GADM1)\n",
    "node_list = assemble_node_list(nuts_nodes, gadm1_nodes, county_nodes)\n",
    "\n",
    "display(node_list.head())\n",
    "print(f\"Totale nodi: {len(node_list):,}\")\n",
    "\n",
    "# 4) Salvataggio\n",
    "OUTPUT_CSV = \"node_list.csv\"\n",
    "node_list.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Salvato: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f3a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
